---
title: "DIPnet Population Structure Notebook"
output: 
  html_notebook:
    toc: yes
---

# Setup
```{r Setup, message=FALSE, warning=FALSE}
library(seqinr)
library(ape)  
library(pegas)
library(hierfstat)
library(mmod)
library(adegenet)
library(plyr)
library(strataG)
library(iNEXT)
library(gdm)
library(gdistance)
library(ecodist)
library(dplyr)
library(reshape2)
library(WriteXLS)
library(ggplot2)
library(knitr)
library(vegan)
library(gridExtra)
library(data.table)


source("config.R")
source("DIPnet_Stats_Functions.R")

rescale<-function(x){
  normalized<-(x-min(x))/(max(x)-min(x))
  return(normalized)
}

remove.dup.gsls<-function(amovatable){
  amovatable<-amovatable[-which(rownames(amovatable) %in% dups),]
}



```

```{r Import Data}
######################################################################
# 1. Import the IPDB and Fst tables
ipdb<-read.table(ipdb_path,sep="\t",header=T,stringsAsFactors = F,quote="", na.strings=c("NA"," ","")) 


#read in geographical regionalizations from Treml
spatial<-read.table(spatial_path, header=T, sep="\t",stringsAsFactors = F, na.strings=c("NA"," ",""), quote="")

#read in geographical regionalizations from Beger
spatial2<-read.table(spatial2_path, header=T,sep="\t", stringsAsFactors = F, na.strings=c("NA"," ",""), quote="")

#read in overwater distances and lookup table from Treml
outMat <-read.table("~/github/IPDB/distances/IPDB_OW.dist.csv", header=TRUE, sep=",", row.names=c(1))
  
LookUp <-read.table("~/github/IPDB/distances/ID_Lookup.csv", header=TRUE, sep=",")

#read in ABGD groups
abgd<-read.table(abgd_path, header=T, sep="\t", stringsAsFactors = F)

#join spatial
ipdb<-join(ipdb,spatial, by = "IPDB_ID",type = "left")
ipdb<-join(ipdb,spatial2[,c(2,18:24)], by = "IPDB_ID", type = "left")

#join ABGD
ipdb<-join(ipdb,abgd[,c(1,3)], by = "IPDB_ID",type = "left")

# drop hybrids and divergent individuals
ipdb<-ipdb[ipdb$IPDB_ID %in% drops == FALSE, ] 



## remove anything not included in the ecoregions scheme (some dolphins, some COTS from Kingman and Madagascar(?), some A. nigros from Kiribati, som C. auriga from Fakareva, hammerheads from Western Australia, and West Africa, and some dolphins from the middle of the eastern tropical pacific

ipdb_ecoregions<-ipdb[-which(is.na(ipdb$ECOREGION)),]

## remove anything that doesn't occur in the 3 Indo-Pacific realms
ipdb_ip<-ipdb_ecoregions[which(ipdb_ecoregions$REALM %in% c("Central Indo-Pacific","Western Indo-Pacific","Eastern Indo-Pacific")),]

amova_ts_path<-"/Users/eric/google_drive/DIPnet_Gait_Lig_Bird/DIPnet_WG4_first_papers/statistics/By_Species/Hierarchical_structure"

amova_abgd_path<-"/Users/eric/google_drive/DIPnet_Gait_Lig_Bird/DIPnet_WG4_first_papers/statistics/By_ESU/Hierarchical_structure"


# Read in a file with taxonomic ordering
taxorder<-read.csv("DIPnet_Species_Class.csv")

#duplicate GSLs that need to be dropped
dups<-c("Acanthaster_planciSIO_CR","Acanthaster_planciRedSea_CR","Acanthaster_planciNIO_CR","Acanthaster_planciPac_CR","Acanthurus_nigrofuscus_CO1","Cephalopholis_argus_CO1","Chaetodon_auriga_CO1","Halichoeres_claudia_CR","Halichoeres_hortulanus_CO1","Halichoeres_hortulanus_CYB","Halichoeres_trimaculatus_CR","Lutjanus_kasmira_CO1","Naso_hexacanthus_CR","Neoniphon_sammara_CO1","Pomacentrus_coelestis_CO1","Pygoplites_diacanthus_CYB", "Stenella_longirostris_CYB")

hypotheses<-c("Bowen","Keith","Kulbicki_r","Kulbicki_b","REALM","PROVINCE","ECOREGION", "VeronDivis")

# Remove dups
ipdb_ip<-ipdb_ip[!ipdb_ip$Genus_species_locus %in% c(dups,"Ctenochaetus_marginatus_A68"),]
```


# Introduction

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. I am trying it out as a way to reproducibly document my work on the DIPnet Population Structure Paper.

The overall workflow for this analysis is as follows:

  - Run AMOVA according to several different regionalizations
    - Briggs and Bowen 2013 - Fish Biogeography
    - Veron et al. 2015 - Coral Biogeography
    - Spalding et al. 2007 - 
        - Marine Ecoregions
        - Provinces
        - Realms
    - Kulbicki et al. 2013 - 
        - Regionalization based on all species, 
        - or just "reliable" species
    - Keith et al. 2013 - Faunal Breaks
  - Select the best model for each species according to the Bayesian Information Criterion
  - Analyze individual "barriers" as implied by the regionalization that best explains the distribution of variation in the most species by modeling pairwise genetic distances given geographic distance and each barrier (individually or together):
    - Distance Based Redundancy Analysis (dbRDA)
  
# Run AMOVA loops

Given the special considerations, we need to run 4 different flavors of AMOVA:
  1. WCTheta and TradSpec
  2. WCTheta and ABGD
  3. PhiST and TradSpec
  4. PhiST and ABGD

Thus, we need to loop through all of the regionalizations above, running all 4 flavors of AMOVA.


```{r AMOVA Loops, eval=F}

## Loop through hypotheses, calculating AMOVA
hypotheses<-c("Kulbicki_r","REALM","Bowen","Kulbicki_b","Keith", "VeronDivis","PROVINCE","ECOREGION")
amova_list<-list()

for(h in hypotheses){
  hierstats<-hierarchical.structure.mtDNA.db(ipdb = ipdb_ip,level1 = "sample",level2=h,model="N",ABGD=F,nperm=1)
  amova_list[[h]]<-hierstats
}
  
#load(file.path(amova_ts_path,"PHIST_TradSpecies_raw_amova_output.Rdata"))
## Summarize AMOVA results
amovastats<-summarize_AMOVA(amova_list,hypotheses,specieslist=unique(ipdb$Genus_species_locus))

WriteXLS(amovastats,ExcelFileName=file.path(amova_ts_path,"PHIST_ts_rawN_table_amova_output.Rdata.xlsx"),row.names = T)
save(amova_list,file=file.path(amova_ts_path,"PHIST_ts_rawN_amova_output.Rdata"))
save(amovastats,file=file.path(amova_ts_path,"PHIST_ts_tableN_amova_output.Rdata"))
```


This takes a long time, so we are not running this code in the document, but loading in the results as we go through each consideration

# Visualize AMOVA results


## PHIST and Traditional Species Boundaries


### Measure Support
```{r Measure Support2}

load(file=file.path(amova_ts_path,"PHIST_ts_tableN_amova_output.Rdata"))
#load(file=file.path(amova_ts_path,"FST_TradSpec_table_amova_output.Rdata"))

#remove duplicate gsls
amovastats<-lapply(amovastats,remove.dup.gsls)

# Measure support for each hypothesis

#Get the values for each hypothesis for a given criterion - here I use BIC - and rank them for #each species, then choose the "best" hypothesis for each species based on the criterion.

criterion<-"BIC"
# find the maximum number of species from any of the 8 hypotheses
maxlength<-max(sapply(amovastats,function(x) length(x[[1]]))) 
# create an empty data frame with row names from the hypothesis with the most values
crit_df<-data.frame(setNames(replicate(maxlength,numeric(0), simplify = F),nm=row.names(amovastats[["PROVINCE"]]))) 

#Loop through the hypotheses, pulling out the values for criterion,transpose it, and then merge these values into the dataframe from the previous hypothesis 
for(h in names(amovastats)){
  crit_df<-merge(crit_df,t(amovastats[[h]][criterion]),all=T,sort=F)
}
#get the hypothesis names in there
row.names(crit_df)<-names(amovastats)

#rank the hypotheses for each species
crit_rank<-as.data.frame(sapply(crit_df,rank,na.last="keep",ties.method="average"))
row.names(crit_rank)<-names(amovastats)

## remove gsls with more than 3 missing models
crit_rank<-crit_rank[, colSums(is.na(crit_rank)) < nrow(crit_rank)-4]  

## choose the best hypothesis or set of hypotheses for each species  
best_hypothesis<-sapply(crit_rank,function(x){row.names(crit_rank)[which(x==min(x,na.rm=T)) ]})

# Convert to Data Frame
best_hypothesis<-as.data.frame(unlist(best_hypothesis))
best_hypothesis$Genus_species_locus<-row.names(best_hypothesis)


#some grepping to get the extra numbers off the ends of gsl names
best_hypothesis$Genus_species_locus<-gsub("(\\w+_\\w+_CR)", replacement="\\1R",best_hypothesis$Genus_species_locus, perl=T)

best_hypothesis$Genus_species_locus<-gsub("(\\w+_\\w+_\\w\\w\\w)\\w", replacement="\\1", best_hypothesis$Genus_species_locus, perl=T)

best_hypothesis$Genus_species_locus<-gsub("(\\w+_\\w+_CR)R", replacement="\\1", best_hypothesis$Genus_species_locus, perl=T)


best_hypothesis<-left_join(best_hypothesis, taxorder[,8:9], by="Genus_species_locus")

names(best_hypothesis)[1]<-"Hypothesis"


#Order the levels of "Taxon"

best_hypothesis$Taxon<-factor(best_hypothesis$Taxon,levels=c("Arthropoda","Mollusca","Echinodermata","Acanthuridae","Chaetodontidae","Holocentridae","Labridae","Lutjanidae","Pomacanthidae","Pomacentridae","Scombridae","Other Fish","Delphinidae"))

# make a custom palette
library(RColorBrewer)
blue<-brewer.pal(9,"Blues")

cols<-c("#7FC97F","#FDC086","#FFFF99","#CDE2FF","#BFD1EC",blue[3:9],"#BEAED4")

#remove Tridacna costata because according to Marc Kochzius, this is actually T. crocea data
best_hypothesis<-best_hypothesis[-77,]
                    
## Make a bar graph of best support for each hypothesis among species

barplot<-ggplot(data=best_hypothesis, aes(x=Hypothesis,fill=Taxon )) + geom_bar(aes(y = (..count..)/sum(..count..))) +
   scale_x_discrete(limits=hypotheses, labels=c("Kulbicki et al. \n Regions k = 3","Spalding et al. \n Realms k = 3","Briggs and Bowen \n k = 5","Kulbicki et al. \n Provinces k = 10","Keith et al. \n k = 11","Veron et al. \n k = 12",  "Spalding et al. \n Provinces k = 27" , "Spalding et al. \n Ecoregions k = 77")) +
  labs(x="Hypothesis",y="Proportion of Species") + theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.title=element_blank()) + scale_fill_manual(values=cols)

barplot

#ggsave("./output/PHIST_Bars_Final.pdf", barplot, width=7,height=7,units="in")
```


### Calculate relative probability from Johnson and Omland 2004
```{r Relative Probability Heatmap2, echo=T}
# lookup the minimum BIC value for each species (which.min works better here, because it returns only the first instance of the minimum value)
minBIC<-sapply(crit_df,function(x){x[which.min(x)]}) 

#J&O box 4 eqn 1. scale() seems to be the way to go here, using the minBIC as the centering vector
crit_df_deltaI<-scale(crit_df, center=minBIC, scale=F) 

#J&O box 4 eqn 4. numerator, plus make it a data frame
crit_df_deltaI_b<-as.data.frame(exp(-0.5*crit_df_deltaI)) 

#J&O box 4 eqn 4. denominator
crit_df_deltaI_sums<-sapply(crit_df_deltaI_b,sum,na.rm=T) 

# this time use the scale argument of scale to divide each column by the corresponding sum
crit_df_relative_prob<-scale(crit_df_deltaI_b,center=F,scale=crit_df_deltaI_sums) 

# Now zap all GSLs that test less than 5 hypotheses

naCount<-colSums(is.na(crit_df_relative_prob))

na4<-names(naCount)[which(naCount<4)]


crit_df_relative_prob_na4<-crit_df_relative_prob[,na4]

#remove Tridacna costata because Kochzius says this is actually T. crocea data
crit_df_relative_prob_na4<-crit_df_relative_prob_na4[,-51]
```

There are `r length(na4)` GSLs that test at least 5 hypotheses

### Heatmap
So now make a trimmed heatmap of just these.

```{r}
## Make a new heatmap of relative probability of each hypothesis

#melt for ggplot2
relprob<-melt(crit_df_relative_prob_na4)
colnames(relprob)<-c("Hypothesis","Species","Relative_Probability")


#make names
spnames<-sub("(\\w+)_(\\w+)_(\\w+)","\\1 \\2 \\3",na4, perl=T)
spnames<-sort(spnames)

#sort taxonomically
relprob$Species<-factor(relprob$Species,levels=taxorder$Genus_species_locus)

#baseplot
rp<-ggplot(relprob,aes(y=Species,x=Hypothesis,fill=Relative_Probability))

#add geom_tile, turn the x-axis elements by 90 degrees, reverse the names on the y-axis, and use a diverging color scheme to highlight hypotheses with >50% rel prob.
rp<-rp+geom_tile()+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  scale_fill_gradient(low = "white",
                       high = "red", space = "rgb",
                       na.value = "grey50", guide = "colourbar") +
  scale_x_discrete(limits=hypotheses, labels=c("Kulbicki et al. \n Regions k = 3","Spalding et al. \n Realms k = 3","Briggs and Bowen \n k = 5","Kulbicki et al. \n Provinces k = 10","Keith et al. \n k = 11","Veron et al. \n k = 12",  "Spalding et al. \n Provinces k = 27" , "Spalding et al. \n Ecoregions k = 77")) +
  scale_y_discrete(limits=rev(levels(relprob$Species)), labels=sub("(\\w+)_(\\w+)_(\\w+)","\\1 \\2 \\3",rev(levels(relprob$Species)), perl=T))
rp

#save this from the viewer

#ggsave("./output/PhiST_Heatmap_Final.pdf", rp)
```

### Calculate Effect Sizes

Sean Connolly wants to know if the tendency for the analysis to favor fewer regions is a consequence of the smaller sample sizes (and thus lower power) relative to number of parameters for the more finely subdivided bioregions (i.e. number of populations per bioregion would be smaller). To do this, I am going to look at PhiCT (variance among regions/total variance) as well as Sigma^2a (variance among regions) instead of BIC.

#### PHICT
```{r Effect Sizes}



# Look at mean PhiCT for each hypothesis
FCTs<-NULL
for(h in hypotheses){
  FCT<-amovastats[[h]]$FCT
  FCTs[[h]]<-FCT
}

FCTsmelt<-melt(FCTs)

ggplot(FCTsmelt,aes(x=L1, y=value)) + geom_boxplot() + scale_x_discrete(limits=hypotheses, labels=c("Kulbicki et al. \n Regions k = 3","Spalding et al. \n Realms k = 3","Briggs and Bowen \n k = 5","Kulbicki et al. \n Provinces k = 10","Keith et al. \n k = 11","Veron et al. \n k = 12",  "Spalding et al. \n Provinces k = 27" , "Spalding et al. \n Ecoregions k = 77")) + theme(axis.text.x = element_text(angle = 90, hjust = 1))+
                                        ylab("Phi_CT") + xlab("Regionalization")

lapply(FCTs,mean)

effect<-data.frame(setNames(replicate(maxlength,numeric(0), simplify = F),nm=row.names(amovastats[["PROVINCE"]]))) 

for(h in names(amovastats)){
 # effect<-merge(effect,t(amovastats[[h]]["level2_SSD"]/amovastats[[h]]["total_SSD"]),all=T,sort=F) #rsquared or etasquared
  effect<-merge(effect,t(amovastats[[h]]["FCT"]),all=T,sort=F)
}

row.names(effect)<-names(amovastats)

effect$model<-rownames(effect)

effectsize<-melt(effect)
colnames(effectsize)<-c("Hypothesis","Species","Effect_Size")


#baseplot
ef<-ggplot(effectsize,aes(y=Species,x=Hypothesis,fill=Effect_Size))

#add geom_tile, turn the x-axis elements by 90 degrees, reverse the names on the y-axis, and use a diverging color scheme to highlight hypotheses with >50% rel prob.
ef<-ef+geom_tile()+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  scale_fill_gradient2(low = "blue",mid="white",
                       high = "red", space = "rgb",
                       na.value = "grey50", guide = "colourbar") +
  scale_x_discrete(limits=hypotheses, labels=c("Kulbicki et al. \n Regions k = 3","Spalding et al. \n Realms k = 3","Briggs and Bowen \n k = 5","Kulbicki et al. \n Provinces k = 10","Keith et al. \n k = 11","Veron et al. \n k = 12",  "Spalding et al. \n Provinces k = 27" , "Spalding et al. \n Ecoregions k = 77")) +
  scale_y_discrete(limits=rev(levels(effectsize$Species)), labels=sub("(\\w+)_(\\w+)_(\\w+)","\\1 \\2 \\3",rev(levels(effectsize$Species)), perl=T))


effect$model<-NULL
#Change all values to negative so that rank() treats them appropriately
effectneg<-as.data.frame(sapply(effect,function(x){return(x*-1)}))
rownames(effectneg)<-rownames(effect)
#rank the hypotheses for each species
effect_rank<-as.data.frame(sapply(effectneg,rank,na.last="keep",ties.method="average"))
rownames(effect_rank)<-rownames(effectneg)

## remove gsls with more than 3 missing models
effect_rank<-effect_rank[, colSums(is.na(effect_rank)) < nrow(effect_rank)-4]  

## choose the best hypothesis or set of hypotheses for each species  
best_hypothesis<-sapply(effect_rank,function(x){row.names(effect_rank)[which(x==min(x,na.rm=T)) ]})



# Convert to Data Frame
best_hypothesis<-as.data.frame(unlist(best_hypothesis))
best_hypothesis$Genus_species_locus<-row.names(best_hypothesis)


#some grepping to get the extra numbers off the ends of gsl names
best_hypothesis$Genus_species_locus<-gsub("(\\w+_\\w+_CR)", replacement="\\1R",best_hypothesis$Genus_species_locus, perl=T)

best_hypothesis$Genus_species_locus<-gsub("(\\w+_\\w+_\\w\\w\\w)\\w", replacement="\\1", best_hypothesis$Genus_species_locus, perl=T)

best_hypothesis$Genus_species_locus<-gsub("(\\w+_\\w+_CR)R", replacement="\\1", best_hypothesis$Genus_species_locus, perl=T)


best_hypothesis<-left_join(best_hypothesis, taxorder[,8:9], by="Genus_species_locus")

names(best_hypothesis)[1]<-"Hypothesis"


#Order the levels of "Taxon"

best_hypothesis$Taxon<-factor(best_hypothesis$Taxon,levels=c("Arthropoda","Mollusca","Echinodermata","Acanthuridae","Chaetodontidae","Holocentridae","Labridae","Lutjanidae","Pomacanthidae","Pomacentridae","Scombridae","Other Fish","Delphinidae"))

# make a custom palette
library(RColorBrewer)
blue<-brewer.pal(9,"Blues")

cols<-c("#7FC97F","#FDC086","#FFFF99","#CDE2FF","#BFD1EC",blue[3:9],"#BEAED4")
                    
## Make a bar graph of best support for each hypothesis among species

barplot<-ggplot(data=best_hypothesis, aes(x=Hypothesis,fill=Taxon )) + geom_bar(aes(y = (..count..)/sum(..count..))) +
   scale_x_discrete(limits=hypotheses, labels=c("Kulbicki et al. \n Regions k = 3","Spalding et al. \n Realms k = 3","Briggs and Bowen \n k = 5","Kulbicki et al. \n Provinces k = 10","Keith et al. \n k = 11","Veron et al. \n k = 12",  "Spalding et al. \n Provinces k = 27" , "Spalding et al. \n Ecoregions k = 77")) +
  labs(x="Hypothesis",y="Proportion of Species") + theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.title=element_blank()) + scale_fill_manual(values=cols)

barplot



```

#### Sigma^2a (level2_sigma2)

```{r}

sigma_as<-NULL
for(h in hypotheses){
  sigma_a<-amovastats[[h]]$level2_sigma2
  sigma_as[[h]]<-sigma_a
}

sigma_as_melt<-melt(sigma_as)

ggplot(sigma_as_melt,aes(x=L1, y=value)) + geom_boxplot() + scale_x_discrete(limits=hypotheses, labels=c("Kulbicki et al. \n Regions k = 3","Spalding et al. \n Realms k = 3","Briggs and Bowen \n k = 5","Kulbicki et al. \n Provinces k = 10","Keith et al. \n k = 11","Veron et al. \n k = 12",  "Spalding et al. \n Provinces k = 27" , "Spalding et al. \n Ecoregions k = 77")) + theme(axis.text.x = element_text(angle = 90, hjust = 1))+
                                        ylab("Sigma^2_regions") + xlab("Regionalization")

lapply(sigma_as,mean)


effect<-data.frame(setNames(replicate(maxlength,numeric(0), simplify = F),nm=row.names(amovastats[["PROVINCE"]]))) 

for(h in names(amovastats)){
 # effect<-merge(effect,t(amovastats[[h]]["level2_SSD"]/amovastats[[h]]["total_SSD"]),all=T,sort=F) #rsquared or etasquared
  effect<-merge(effect,t(amovastats[[h]]["level2_sigma2"]),all=T,sort=F)
}

effect$model<-NULL
#Change all values to negative so that rank() treats them appropriately
effectneg<-as.data.frame(sapply(effect,function(x){return(x*-1)}))
rownames(effectneg)<-rownames(effect)
#rank the hypotheses for each species
effect_rank<-as.data.frame(sapply(effectneg,rank,na.last="keep",ties.method="average"))
rownames(effect_rank)<-rownames(effectneg)

## remove gsls with more than 3 missing models
effect_rank<-effect_rank[, colSums(is.na(effect_rank)) < nrow(effect_rank)-4]  

## choose the best hypothesis or set of hypotheses for each species  
best_hypothesis<-sapply(effect_rank,function(x){row.names(effect_rank)[which(x==min(x,na.rm=T)) ]})



# Convert to Data Frame
best_hypothesis<-as.data.frame(unlist(best_hypothesis))
best_hypothesis$Genus_species_locus<-row.names(best_hypothesis)


#some grepping to get the extra numbers off the ends of gsl names
best_hypothesis$Genus_species_locus<-gsub("(\\w+_\\w+_CR)", replacement="\\1R",best_hypothesis$Genus_species_locus, perl=T)

best_hypothesis$Genus_species_locus<-gsub("(\\w+_\\w+_\\w\\w\\w)\\w", replacement="\\1", best_hypothesis$Genus_species_locus, perl=T)

best_hypothesis$Genus_species_locus<-gsub("(\\w+_\\w+_CR)R", replacement="\\1", best_hypothesis$Genus_species_locus, perl=T)


best_hypothesis<-left_join(best_hypothesis, taxorder[,8:9], by="Genus_species_locus")

names(best_hypothesis)[1]<-"Hypothesis"


#Order the levels of "Taxon"

best_hypothesis$Taxon<-factor(best_hypothesis$Taxon,levels=c("Arthropoda","Mollusca","Echinodermata","Acanthuridae","Chaetodontidae","Holocentridae","Labridae","Lutjanidae","Pomacanthidae","Pomacentridae","Scombridae","Other Fish","Delphinidae"))

# make a custom palette
library(RColorBrewer)
blue<-brewer.pal(9,"Blues")

cols<-c("#7FC97F","#FDC086","#FFFF99","#CDE2FF","#BFD1EC",blue[3:9],"#BEAED4")
                    
## Make a bar graph of best support for each hypothesis among species

barplot<-ggplot(data=best_hypothesis, aes(x=Hypothesis,fill=Taxon )) + geom_bar(aes(y = (..count..)/sum(..count..))) +
   scale_x_discrete(limits=hypotheses, labels=c("Kulbicki et al. \n Regions k = 3","Spalding et al. \n Realms k = 3","Briggs and Bowen \n k = 5","Kulbicki et al. \n Provinces k = 10","Keith et al. \n k = 11","Veron et al. \n k = 12",  "Spalding et al. \n Provinces k = 27" , "Spalding et al. \n Ecoregions k = 77")) +
  labs(x="Hypothesis",y="Proportion of Species") + theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.title=element_blank()) + scale_fill_manual(values=cols)

barplot

```

## FST and Traditional Species Boundaries


### Measure Support
```{r Measure Support1, echo=F}
load(file=file.path(amova_ts_path,"FST_TradSpec_table_amova_output.Rdata"))
#remove duplicate gsls
amovastats<-lapply(amovastats,remove.dup.gsls)




# Measure support for each hypothesis

#Get the values for each hypothesis for a given criterion - here I use BIC - and rank them for #each species, then choose the "best" hypothesis for each species based on the criterion.

criterion<-"BIC"
# find the maximum number of species from any of the 8 hypotheses
maxlength<-max(sapply(amovastats,function(x) length(x[[1]]))) 
# create an empty data frame with row names from the hypothesis with the most values
crit_df<-data.frame(setNames(replicate(maxlength,numeric(0), simplify = F),nm=row.names(amovastats[["PROVINCE"]]))) 

#Loop through the hypotheses, pulling out the values for criterion,transpose it, and then merge these values into the dataframe from the previous hypothesis 
for(h in names(amovastats)){
  crit_df<-merge(crit_df,t(amovastats[[h]][criterion]),all=T,sort=F)
}
#get the hypothesis names in there
row.names(crit_df)<-names(amovastats)

#rank the hypotheses for each species
crit_rank<-as.data.frame(sapply(crit_df,rank,na.last="keep",ties.method="average"))
row.names(crit_rank)<-names(amovastats)

## remove gsls with more than 3 missing models
crit_rank<-crit_rank[, colSums(is.na(crit_rank)) < nrow(crit_rank)-4]  

## choose the best hypothesis or set of hypotheses for each species  
best_hypothesis<-sapply(crit_rank,function(x){row.names(crit_rank)[which(x==min(x,na.rm=T)) ]})


# Add phylum to each
best_hypothesis<-as.data.frame(unlist(best_hypothesis))
best_hypothesis$Genus_species_locus<-row.names(best_hypothesis)
ipdb_phylum<-ipdb_ip[,c("Genus_species_locus","phylum")]
ipdb_phylum<-unique(ipdb_phylum)

#some grepping to get the extra numbers off the ends of gsl names
best_hypothesis$Genus_species_locus<-gsub("(\\w+_\\w+_CR)", replacement="\\1R",best_hypothesis$Genus_species_locus, perl=T)

best_hypothesis$Genus_species_locus<-gsub("(\\w+_\\w+_\\w\\w\\w)\\w", replacement="\\1", best_hypothesis$Genus_species_locus, perl=T)

best_hypothesis$Genus_species_locus<-gsub("(\\w+_\\w+_CR)R", replacement="\\1", best_hypothesis$Genus_species_locus, perl=T)


best_hypothesis<-left_join(best_hypothesis, ipdb_phylum, by="Genus_species_locus")

names(best_hypothesis)[1]<-"Hypothesis"

                               
## Make a bar graph of best support for each hypothesis among species

barplot<-ggplot(data=best_hypothesis, aes(x=Hypothesis,fill=phylum )) + geom_bar(aes(y = (..count..)/sum(..count..))) +
   scale_x_discrete(limits=hypotheses, labels=c("Briggs and Bowen \n k = 5","Keith et al. \n k = 11", "Kulbicki et al. \n Regions k = 3", "Kulbicki et al. \n Provinces k = 10", "Spalding et al. \n Realms k = 3", "Spalding et al. \n Provinces k = 27" , "Spalding et al. \n Ecoregions k = 25", "Veron et al. \n k = 12")) +
  labs(x="Hypothesis",y="Proportion of Species") + theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.title=element_blank())

barplot

#ggsave("./output/FST_Bars_Final.pdf", barplot,width=7,height=7,units="in")
```

### Calculate relative probability from Johnson and Omland 2004 and visualize with a heatmap
```{r Relative Probability Heatmap1, fig.height=11, fig.width=8.5, echo=F}
# lookup the minimum BIC value for each species (which.min works better here, because it returns only the first instance of the minimum value)
minBIC<-sapply(crit_df,function(x){x[which.min(x)]}) 

#J&O box 4 eqn 1. scale() seems to be the way to go here, using the minBIC as the centering vector
crit_df_deltaI<-scale(crit_df, center=minBIC, scale=F) 

#J&O box 4 eqn 4. numerator, plus make it a data frame
crit_df_deltaI_b<-as.data.frame(exp(-0.5*crit_df_deltaI)) 

#J&O box 4 eqn 4. denominator
crit_df_deltaI_sums<-sapply(crit_df_deltaI_b,sum,na.rm=T) 

# this time use the scale argument of scale to divide each column by the corresponding sum
crit_df_relative_prob<-scale(crit_df_deltaI_b,center=F,scale=crit_df_deltaI_sums) 

# Now zap all GSLs that test less than 5 hypotheses

naCount<-colSums(is.na(crit_df_relative_prob))

na4<-names(naCount)[which(naCount<4)]


crit_df_relative_prob_na4<-crit_df_relative_prob[,na4]

## Make a heatmap of relative probability of each hypothesis

#melt for ggplot2
relprob<-melt(crit_df_relative_prob_na4)
colnames(relprob)<-c("Hypothesis","Species","Relative_Probability")

#make names
spnames<-sub("(\\w+)_(\\w+)_\\w+","\\1 \\2",na4, perl=T)
spnames<-sort(spnames)

#baseplot
rp<-ggplot(relprob,aes(y=Species,x=Hypothesis,fill=Relative_Probability))

#add geom_tile, turn the x-axis elements by 90 degrees, reverse the names on the y-axis, and use a diverging color scheme to highlight hypotheses with >50% rel prob.
rp<-rp+geom_tile()+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  scale_fill_gradient(low = "white",
                       high = "red", space = "rgb",
                       na.value = "grey50", guide = "colourbar") +
  scale_x_discrete(limits=hypotheses, labels=c("Briggs and Bowen \n k = 5","Keith et al. \n k = 11", "Kulbicki et al. \n Regions k = 3", "Kulbicki et al. \n Provinces k = 10", "Spalding et al. \n Realms k = 3", "Spalding et al. \n Provinces k = 27" , "Spalding et al. \n Ecoregions k = 25", "Veron et al. \n k = 12")) +
  scale_y_discrete(limits=rev(sort(levels(relprob$Species))), labels=rev(spnames))
rp

ggsave("./output/FST_Heatmap_Final.pdf", rp, width=7, height=7, units = "in")
```



# Calculate Pairwise Differentiation Stats

By sample

```{r, eval=F, echo=T}
diffstats<-pairwise.structure.mtDNA.db(ipdb=ipdb, gdist = "WC Theta", minseqs = 5, minsamps = 3, mintotalseqs = 0, nrep = 0, num.cores = 2, ABGD = T, regionalization = "sample")


save(diffstats, file="/Users/eric/google_drive/DIPnet_Gait_Lig_Bird/DIPnet_WG4_first_papers/statistics/By_ESU/Pairwise_statistics/DIPnet_structure_sample_WCTheta_ABGD.Rdata")

diffstats<-pairwise.structure.mtDNA.db(ipdb=ipdb, gdist = "Jost D", minseqs = 5, minsamps = 3, mintotalseqs = 0, nrep = 0, num.cores = 2, ABGD = T, regionalization = "sample")


save(diffstats, file="/Users/eric/google_drive/DIPnet_Gait_Lig_Bird/DIPnet_WG4_first_papers/statistics/By_ESU/Pairwise_statistics/DIPnet_structure_sample_JostD_ABGD.Rdata")
```



# Mixing Bowen and Spalding Realms
First step is to distinguish Hawaii, Easter Island and the Marquesas, thereby mixing in a little bit of Briggs and Bowen


```{r}
unique(ipdb_ip$Bowen)

unique(ipdb_ip$REALM)

dim(ipdb_ip)


ipdb_ip$REALM[which(ipdb_ip$Bowen=="Hawaiian")]<-"Hawaii"
ipdb_ip$REALM[which(ipdb_ip$Bowen=="Marquesas")]<-"Marquesas"
ipdb_ip$REALM[which(ipdb_ip$Bowen=="Easter")]<-"Easter"
ipdb_ip$REALM[which(ipdb_ip$Bowen=="Western Indian Ocean Province")]<-"Western Indian Ocean Province"

unique(ipdb_ip$REALM)


```

# dbRDA

```{r dbRDA}
# read in the Fst/PhiSt table 
load("~/google_drive/DIPnet_Gait_Lig_Bird/DIPnet_WG4_first_papers/statistics/By_Species/Pairwise_statistics/sample/DIPnet_structure_sample_PhiST.Rdata")
#load("~/Desktop/DIPnet_structure_sample_PhiST_042817.Rdata")

# read in the best hypotheses
best_hypotheses<-read.csv("best_hypothesis.csv")

##1. Dataframe for results
stats<-data.frame(Species_Locus=character(0),constrained.inertia=numeric(0),totalInertia=numeric(0),ProportionConstrained=numeric(0),adj.R2.total=numeric(0),modelF=numeric(0),modelPvalue=numeric(0),pcx_Var=numeric(0),pcx_p=numeric(0),pcy_Var=numeric(0),pcy_p=numeric(0),bestmodel=character(0), constrained.inertia.best=numeric(0), total.inertia.best=numeric(0), proportion.constrained.inertia.best=numeric(0), adj.R2.best.model=numeric(0), stringsAsFactors = F)
stats$Species_Locus<-as.character(stats$Species_Locus)
#stats$Barrier<-as.character(stats$Barrier)
stats$bestmodel<-as.character(stats$bestmodel)

plotlist<-list()

# Make an empty list to save rda output for each species
esu_loci <- na4
all.gsl.rda<-sapply(esu_loci, function(x) NULL)

#make empty data frames to save variance and pvalues
term.vars<-data.frame(gsl=character(0))
term.pvals<-data.frame(gsl=character(0))

###############################################################################
# 2. Subsample for each species of interest, and filter based on Phi_ST table.
for(gsl in esu_loci){ #gsl<-"Linckia_laevigata_CO1" "Tridacna_crocea_CO1" "Lutjanus_kasmira_CYB"
  
  cat("\n","\n","\n","Now starting", gsl, "\n")
  
  #lookup the best hypothesis
  #best_h<-as.character(best_hypotheses[which(gsl==best_hypotheses$gsl),2])
  best_h<-"REALM"
#  if(any(is.na(diffstats[[gsl]]))){cat("NAs in FST table, No dbRDA calculated"); #all.gsl.rda[[gsl]]<-"NAs in FST table, No dbRDA calculated"; next}
 
  #replace NAs with 1s. NAs occur when no haplotypes are shared.
   if(any(is.na(diffstats[[gsl]]))){
    diffstats[[gsl]][which(is.na(diffstats[[gsl]]))]<-1
  }
   
  if(diffstats[[gsl]]=="Fewer than 3 sampled populations after filtering. No stats calculated"){all.gsl.rda[[gsl]]<-"Fewer than 4 sampled populations after filtering."; cat("Fewer than 4 sampled populations after filtering.");next}
  
   sp<-ipdb_ip[which(ipdb_ip$Genus_species_locus==gsl),]
  
  #clean weird backslashes from names
  sp$locality<-gsub("\"","",sp$locality)
  
  sp$sample<-paste(sp$locality,round(sp$decimalLatitude, digits=0),round(sp$decimalLongitude, digits=0),sep="_")  #sets up a variable that matches the name in Fst table
  sp<-sp[order(sp$sample),]
  # Not all localities are included in every realization - zap the ones that are NA
  #nonpops<-unique(sp$sample[is.na(sp$REALM)])
  sp<-sp[!is.na(sp[,best_h]),]
  
  #subsample Fst 
  gslFST<-diffstats[[gsl]]
  #make a matrix out of gslFST, convert negative values to zero
  gslFSTm<-as.matrix(gslFST)
  gslFSTm[which(gslFSTm<0)]<-0.0
  
  #zap weird slashes in the names
  rownames(gslFSTm)<-gsub("\"","",rownames(gslFSTm))
  colnames(gslFSTm)<-rownames(gslFSTm)
  
  #zap the same na populations from the list of non existent pops from VeronDivis
  # if(any(rownames(gslFSTm) %in% nonVeronpops)){
  #   gslFSTm<-gslFSTm[-(which(rownames(gslFSTm) %in% nonVeronpops)),-(which(colnames(gslFSTm) %in% nonVeronpops))]
  # }
  if(length(rownames(gslFSTm))<5){
    all.gsl.rda[[gsl]]<-"Fewer than 5 sampled populations"
    cat("Fewer than 5 sampled populations")
    next}
  
  #and filter sp based on the localities that have Fst values
  sp<-sp[sp$sample %in% rownames(gslFSTm),]
  
  #and vice versa
  
  gslFSTm<- gslFSTm[which(rownames(gslFSTm) %in% unique(sp$sample)),which(rownames(gslFSTm) %in% unique(sp$sample))]
  

  #if(length(rownames(gslFSTm))<5){all.gsl.rda[[gsl]]<-"Fewer than 5 sampled populations";cat("Fewer than 5 sampled populations");next}
  
  #create a locations data frame that has all the localities plus lats and longs and their Veron region.
  locs<-as.data.frame(unique(sp$sample))
  names(locs)<-"sample"
  #locs$Long<-sp$decimalLongitude[which(locs %in% sp$sample)]
  #can't do a unique on sample, lats and longs because some samples have non-unique lats and longs! So I do a join and take the first match.
  locs<-join(locs,sp[c("IPDB_index","sample","decimalLongitude","decimalLatitude","REALM","PROVINCE","ECOREGION","VeronDivis","Kulbicki_b","Kulbicki_r","Bowen","Keith")], by="sample", match="first")
  #if (length(unique(locs$REALM)) < 2) {"Only one region!"; cat("Only one region!"); next}
  
  #sort gslFSTm
  gslFSTm<-gslFSTm[order(rownames(gslFSTm)),order(colnames(gslFSTm))]

  
  ######################################################################
  # 4. Calculate Great Circle Distance
  gcdist_km <- pointDistance(locs[,3:4],lonlat=T)/1000
  # and symmetricise it
  gcdist_km[upper.tri(gcdist_km)]<-0
  gcdist_km<-gcdist_km + t(gcdist_km)
   
  ####################################################################### Calculate Overwater Distances#
  #Save for later##
   mySites<-locs$IPDB_index
  
  SubSet.Sites<-setDT(LookUp, key = 'IPDB_index')[list(mySites)]

  myIndex<-SubSet.Sites$Rid
  
  # some distances were not measurable due to point being 20km from water
  # zap these points from both matrices
  #rownames(mySubOutMat)<-mySites
  #colnames(mySubOutMat)<-mySites
  if(anyNA(myIndex)){
  nazaps<-which(is.na(myIndex))
  myIndex<-myIndex[-nazaps]
  gcdist_km<-gcdist_km[-nazaps,-(nazaps)]
  gslFSTm<-gslFSTm[-nazaps,-(nazaps)]
  locs<-locs[-nazaps,]
 
  }
  
  owdist_km<-outMat[myIndex,myIndex]
  
  #add the sample names
  rownames(owdist_km) <- locs$sample
  
 

  ##############################################################################
  #5. Create a matrix of regional identities
  if(length(unique(locs[,best_h]))==1){
    name<-gsub(unique(locs[,best_h]),pattern = "[ -]", replacement=".")
    name<-paste0("get.best_h.",name)
    regions<-data.frame(name=rep(1,length(locs$sample)))
    colnames(regions)<-name
    }
  else{
    regions<-with(locs, data.frame(model.matrix(~get(best_h)+0)))   #one of the predictors is superfluous but will get knocked out during RDA
    row.names(regions)<-row.names(locs)
  }
  
  #################################################################################
  #6. Plot IBD
  
  ibd<-as.data.frame(cbind(distance=as.dist(owdist_km), phist=as.dist(gslFSTm)))
  
  ibdplot<- ggplot(data=ibd,mapping=aes(x=distance,y=phist)) + geom_point() +
      geom_smooth(method=lm, color="purple") +
      ggtitle(label=gsl)

plotlist[[gsl]]<-ibdplot

    ############################################################################
    # 7. Calculate the principal coordinates
    
    FST.pcoa<-cmdscale(gslFSTm, k=dim(as.matrix(gslFSTm))[1] - 1, eig=TRUE, add=FALSE) #ignore warnings - OK to have negatives according to Anderson
    FST.scores<-FST.pcoa$points
    
    owdist.pcoa<-cmdscale(owdist_km, k=2, eig=TRUE, add=FALSE)
    owdist.scores<-owdist.pcoa$points
    owdist.scores<-data.frame("pcx"=owdist.pcoa$points[,1],"pcy"=owdist.pcoa$points[,2])
    locs2<-cbind(regions,owdist.scores)
    
    ###########################################################################
    # 8. Calculate the RDA and extract statistics
    RDA.res<-rda(FST.scores~., data=locs2, scale=TRUE )
    
    #Extract Statistics
    constrained.inertia<-summary(RDA.res)$constr.chi
    total.inertia<-summary(RDA.res)$tot.chi
    proportion.constrained.inertia<-constrained.inertia/total.inertia
    
    adj.R2.total.model<-RsquareAdj(RDA.res)$adj.r.squared
    if(is.na(adj.R2.total.model)){cat("Predictors >= Observations! No solution");all.gsl.rda[[gsl]]<-"Predictors >= Observations! No solution";next}
    
    model.sig<-anova.cca(RDA.res, step=1000)
    modelF<-model.sig$F[1]
    modelPvalue<-model.sig$`Pr(>F)`[1]
    
    ##Uncomment this section to harvest variances from the full model###
    # terms.sig<-anova(RDA.res, by="term", step=1000)
    # pcx_Var<-terms.sig$Variance[1]
    # pcx_p<-terms.sig$`Pr(>F)`[1]
    # pcy_Var<-terms.sig$Variance[2]
    # pcy_p<-terms.sig$`Pr(>F)`[2]
    # 
    # #extract all variance values into a dataframe
    # term.var<-as.data.frame(t(terms.sig[,2]))
    # names(term.var)<-rownames(terms.sig)
    # 
    # #extract all p-values into a dataframe
    # term.pval<-as.data.frame(t(terms.sig[,4]))
    # names(term.pval)<-rownames(terms.sig)
    # 
    #  
    # # scale the variance
    # term.var<-term.var/sum(term.var)
    # 
    # # set variance to zero if it is not significant
    # term.var[1,which(term.pval[1,] > 0.05 | is.na(term.pval[1,]))]<-0
    # term.var$Residual <- 1 - sum(term.var)
    # 
    # #set all values to zero if the model itself is not significant
    # #if(modelPvalue > 0.05){
    # #term.var[1,which(names(term.var)!="gsl")]<-0
    # #}
    # # add on the species name
    # term.pval$gsl<-gsl
    # term.var$gsl<-gsl
    # 
    # #merge them on to each dataframe
    # term.vars<-merge(term.vars,term.var,all=T)
    # term.pvals<-merge(term.pvals,term.pval,all=T)
    ##Uncomment this section to harvest variances from the full model###
    
    
    #barrier_Var<-terms.sig$Variance[3]   #omitting for now as the number of barriers will differ
    #barrier_p<-terms.sig$`Pr(>F)`[3]
    
    #marg.sig<-anova(RDA.res, by="margin", step=1000)
    #barrier_margVar<-marg.sig$Variance[3]
    #barrier_margp<-marg.sig$`Pr(>F)`[3]
    
    #Model selection
    nullmodel<-rda(FST.scores~1, data=locs2, scale=TRUE )
    forward.model<-ordiR2step(nullmodel, scope=formula(RDA.res), R2scope=FALSE, psteps=1000)  #ordiR2step implements Blanchets stopping criterion
    bestmodel<-as.character(forward.model$call[2])
    if (is.null(summary(forward.model)$constr.chi)) {
      constrained.inertia.best<-NA
      total.inertia.best<-summary(forward.model)$tot.chi
      proportion.constrained.inertia.best<-NA
      adj.R2.best.model<-NA
    } else {
      constrained.inertia.best<-summary(forward.model)$constr.chi
      total.inertia.best<-summary(forward.model)$tot.chi
      proportion.constrained.inertia.best<-constrained.inertia.best/total.inertia.best
      adj.R2.best.model<-RsquareAdj(forward.model)$adj.r.squared
    }

    
#### Uncomment this section to harvest variances from the model selection model
    
    terms.sig<-anova(forward.model, by="term", step=1000)
    pcx_Var<-terms.sig$Variance[1]
    pcx_p<-terms.sig$`Pr(>F)`[1]
    pcy_Var<-terms.sig$Variance[2]
    pcy_p<-terms.sig$`Pr(>F)`[2]
    
    #extract all variance values into a dataframe
    term.var<-as.data.frame(t(terms.sig[,2]))
    names(term.var)<-rownames(terms.sig)
   
    #extract all p-values into a dataframe
    term.pval<-as.data.frame(t(terms.sig[,4]))
    names(term.pval)<-rownames(terms.sig)
    
     
    # scale the variance
    term.var<-term.var/sum(term.var)
    
    # add on the species name
    term.pval$gsl<-gsl
    term.var$gsl<-gsl
    
    #merge them on to each dataframe
    term.vars<-merge(term.vars,term.var,all=T)
    term.pvals<-merge(term.pvals,term.pval,all=T)

####Uncomment this section to harvest variances from the model selection model ####
  
    #save stats
    
    stats_model<-c(gsl,constrained.inertia,total.inertia,proportion.constrained.inertia,adj.R2.total.model,modelF,modelPvalue,pcx_Var,pcx_p,pcy_Var,pcy_p, bestmodel, constrained.inertia.best, total.inertia.best, proportion.constrained.inertia.best, adj.R2.best.model)
    
    stats[nrow(stats)+1,]<-stats_model
    
    all.gsl.rda[[gsl]]<-forward.model
    
    
 
  }

write.csv(term.vars,file="./output/dbRDA_term_vars_PHIST_bowenrealms_041318.csv")
write.csv(term.pvals,file="./output/dbRDA_term_pvals_PHIST_bowenrealms_041318.csv")

 save(all.gsl.rda,file="./output/multibarrier_dbRDA_PHIST_bowenrealms_041318.Rdata")
  
write.csv(stats, "./output/multibarrier_dbRDA_PHIST_bowenrealms_041318.csv")

ibdplots<-marrangeGrob(plotlist, nrow=2,ncol=2)
ggsave("./output/ibdplots_PHIST_bowenrealms_041318.pdf",ibdplots)
  
```

## Make a heatmap of the variance values


```{r}


#melt for ggplot2
variance_melted<-melt(term.vars,id.vars = "gsl")
colnames(variance_melted)<-c("Species","Variable","Proportion_of_Variance")
variance_melted$Species<-as.factor(variance_melted$Species)


#baseplot
dbrda<-ggplot(variance_melted,aes(y=Species,x=Variable,fill=Proportion_of_Variance))

#add geom_tile, turn the x-axis elements by 90 degrees, reverse the names on the y-axis, and use a diverging color scheme to highlight hypotheses with >50% rel prob.
dbrda<-dbrda+geom_tile()+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+
  ylim(rev(levels(variance_melted$Species)))+
  scale_fill_gradientn(colors=c("white","pink","red"),values = c(0,0.000001,1),
                       na.value = "grey50", guide = "colourbar")

dbrda
```



# Stacked Bars for dbRDA

```{r}

#sum all non distance, non residual variance terms - these will be the structure component.

term.vars$Structure<-rowSums(term.vars[,-which(names(term.vars) %in% c("gsl","pcx","pcy","Residual"))],na.rm=TRUE)

#Subset just the values that will be plotted
term.vars.structure<-term.vars[,c("gsl","Residual","Structure","pcx","pcy")]

#term.vars.structure[which(stats$modelPvalue>0.05),2]<-1
#term.vars.structure[which(stats$modelPvalue>0.05),c(3,4,5)]<-0



#species names
spnames<-sub("(\\w+)_(\\w+)_\\w+","\\1 \\2",term.vars.structure$gsl)

#melt into long format
term.vars.melt<-melt(term.vars.structure, id.vars="gsl")

term.vars.melt$gsl<-factor(term.vars.melt$gsl)
term.vars.melt$gsl<-factor(term.vars.melt$gsl,levels=rev(levels(term.vars.melt$gsl)))

bars<-ggplot(data=term.vars.melt, mapping=aes(x=gsl,y=value,fill=variable)) +
  geom_col() + coord_flip() +
  scale_fill_manual(values=c("Grey50","Green","Blue","Blue4")) +
  scale_x_discrete(labels=rev(spnames)) + xlab(element_blank()) + ylab("Proportion")

ggsave(filename = "./output/dbRDA_stacked_bars_PHI_bowenrealms_041118.pdf",bars,width=7,height=7,units="in")





#now pull out just the significant models
term.vars.structure.sigmodel<-term.vars.structure[which(term.vars.structure$Residual<1),]

#species names
spnames<-sub("(\\w+)_(\\w+)_\\w+","\\1 \\2",term.vars.structure.sigmodel$gsl)

#melt into long format
term.vars.melt<-melt(term.vars.structure.sigmodel, id.vars="gsl")

term.vars.melt$gsl<-factor(term.vars.melt$gsl)
term.vars.melt$gsl<-factor(term.vars.melt$gsl,levels=rev(levels(term.vars.melt$gsl)))

bars<-ggplot(data=term.vars.melt, mapping=aes(x=gsl,y=value,fill=variable)) +
  geom_col() + coord_flip() +
  scale_fill_manual(values=c("Grey50","Green","Blue","Blue4"), labels=c("Unconstrained Inertia","Merged Regionalization","Zonal Distance","Meridional Distance")) + scale_x_discrete(labels=rev(spnames)) + xlab(element_blank()) + ylab("Proportion of Inertia Constrained") + theme(axis.title.y = element_blank(), legend.title = element_blank())

bars

ggsave(filename = "./output/dbRDA_stacked_bars_PHI_bowenrealms_sigonly_041318.pdf",bars,width=7,height=7,units="in")

```

# Barrier Strength Analysis

Now I am going to come up with code to measure the mean PhiST across the merged regionalizations




## Calculate the diffstats
```{r}
#diffstats2<-pairwise.structure.mtDNA.db(ipdb=ipdb_ip, gdist = "PhiST", minseqs = 5, minsamps = 2, mintotalseqs = 0, nrep = 1000, num.cores = 2, ABGD = F, regionalization = "REALM")
#save(diffstats_phist,file="./output/PHIst_for_map.R")
#save(diffstats_fst,file="./output/Fst_for_map.R")

load("./output/PHIst_for_map.R")
#load("./output/FST_for_map.R")

# now 
str(ipdb2)

```

## Function to pull out pairwise values for putative barriers

```{r}
# first filter out all species that had fewer than 2 sampled populations
diffstats3<-diffstats_phist[lapply(diffstats_phist, class) =="dist"]


barrier_extract<-function(x){
  
  a<-as.matrix(x)
 
     if("Western Indian Ocean Province" %in% rownames(a) && "Western Indo-Pacific" %in% rownames(a))
   {west_indian<- a["Western Indian Ocean Province","Western Indo-Pacific"]}
   else{west_indian<- NA}
   
   if("Central Indo-Pacific" %in% rownames(a) && "Western Indo-Pacific" %in% rownames(a))
   {west_central<- a["Central Indo-Pacific","Western Indo-Pacific"]}
   else{west_central<- NA}
  
   if("Central Indo-Pacific" %in% rownames(a) && "Eastern Indo-Pacific" %in% rownames(a))
   {east_central<- a["Central Indo-Pacific","Eastern Indo-Pacific"]}
   else{east_central<- NA}
  
   if("Hawaii" %in% rownames(a) && "Eastern Indo-Pacific" %in% rownames(a))
   {east_hawaii<- a["Hawaii","Eastern Indo-Pacific"]}
   else{east_hawaii<- NA}
   
   if("Marquesas" %in% rownames(a) && "Eastern Indo-Pacific" %in% rownames(a))
   {east_marquesas<- a["Marquesas","Eastern Indo-Pacific"]}
   else{east_marquesas<- NA}
   

   return(c(west_indian,west_central,east_central,east_hawaii,east_marquesas))
}
```

## Simulated Fst
I ran 1000 simulations of a 1e6 population splitting into 2 1e6 populations with either 0 or 10 migrants/generation, 10K generations ago. Couldn't pull the .arp files into strataG, so had to run Phist analysis in Arlequin

```{r}
sims.nomig<-read.table("./sims_for_structure_paper/2PopDNAnorec_0.5_1000/outSumStats_phist.txt",header=T)


simPHIST<-sims.nomig$FST
simPHIST[which(simPHIST<0)]<-0
simPHISTdf<-data.frame(variable="Simulated", value=simPHIST)

```

## Randomized Fst
Randomly sort each locality for each species into two regions and calculate Fst between them.

```{r, echo=F}
ipdb3<-ipdb_ip[which(ipdb_ip$Genus_species_locus %in% names(diffstats3)),]

ipdb3$randomregion<-NA

random_diffstats2<-NULL
random_diffstats_all<-NULL
for(i in 1:10){
#randomly assign all localities to 1 or 2
  for(loc in unique(ipdb3$locality)){
    ipdb3$randomregion[which(ipdb3$locality==loc)]<-sample(c(1,2),size=1)
  }

random_diffstats<-pairwise.structure.mtDNA.db(ipdb=ipdb3, gdist = "WC Theta", minseqs = 5, minsamps = 2, mintotalseqs = 0, nrep = 0, num.cores = 2, ABGD = F, regionalization = "randomregion")

random_diffstats2<-random_diffstats[lapply(random_diffstats, class) =="dist"]

random_diffstats_all<-c(random_diffstats_all,unlist(random_diffstats2))
}

random_diffstats_all_2<-unlist(random_diffstats_all)
random_diffstats_all_2[which(random_diffstats_all_2 < 0)]<-0

random_diffstats_all_3<-data.frame(variable=factor("Randomized"), value = as.numeric(random_diffstats_all_2))

#save(random_diffstats_all,file="./output/Randomized_PHIst.R")
#save(random_diffstats_all_3,file="./output/Randomized_FST.R")

```


## Pull them out and plot them

```{r}
load("./output/Randomized_PHIst.R")

extracted_Fst<-lapply(diffstats3,barrier_extract)

extracted_Fst2<-matrix(unlist(extracted_Fst), ncol=5, byrow=T)

rownames(extracted_Fst2)<-names(diffstats3)

colnames(extracted_Fst2)<-c("West_Indian","West_Central","East_Central","East_Hawaii","East_Marquesas")

#set values less than 0 to 0
extracted_Fst2[which(extracted_Fst2 < 0)] <- 0

extracted_Fst2<-as.data.frame(extracted_Fst2)


extracted_Fst_melt<-melt(extracted_Fst2,measure.vars=names(extracted_Fst2), na.rm=T)

#rbind on the simulated values
extracted_Fst_melt<-rbind(extracted_Fst_melt,simPHISTdf)
#rbind on the randomized values
extracted_Fst_melt<-rbind(extracted_Fst_melt,random_diffstats_all)

#function to add in n values
n_fun <- function(x){
  return(data.frame(y = -0.05, label = paste0("n = ",length(x))))
}

#ggplot(data=extracted_Fst_melt, aes(x=variable,y=value)) + geom_boxplot() + stat_summary(fun.data = n_fun, geom = "text")

phist_violins<-ggplot(data=extracted_Fst_melt, aes(x=variable,y=value, alpha=variable)) + geom_violin(alpha=1) + stat_summary(fun.data = n_fun, geom = "text", alpha=1) + geom_jitter(size=0.0) + ylab(expression(Phi[ST])) + xlab("Barrier") + scale_alpha_manual(values=c(0.5,0.5,0.5,0.5,0.5,0,0), guide=F) + scale_x_discrete(labels=c("Western Indian/ \n Western Indo-Pacific","Western Indo-Pacific / \n Central Indo-Pacific", "Central Indo-Pacific / \n Eastern Indo-Pacific", "Eastern Indo-Pacific / \n Hawaii", "Eastern Indo-Pacific / \n Marquesas", "Simulated","Randomized")) + theme(axis.text.x = element_text(angle=45, hjust=1))


#Function for 10000 bootstraps which take 100 samples from the data with replacement and establish 95%CI
bootstrap_phist<-function(x){
sapply(1:10000,function(y) median(sample(x=x,size=100,replace=T),na.rm=T))
}


medians<-c(apply(extracted_Fst2,MARGIN=2,FUN=median, na.rm=T),Simulated=median(simPHISTdf$value,na.rm=T), Randomized=median(random_diffstats_all$value,na.rm=T))

medians_dist<-cbind(apply(extracted_Fst2,MARGIN=2,FUN=bootstrap_phist),Simulated=bootstrap_phist(simPHISTdf$value),Randomized=bootstrap_phist(random_diffstats_all$value))

CI95<-apply(medians_dist, MARGIN=2, FUN=quantile,probs=c(0.025,0.975))

#Significance Testing
#West_Indian
sum(medians_dist[,1] < medians_dist[,7])/10000
#West_Central
sum(medians_dist[,2] < medians_dist[,7])/10000
#East_Central
sum(medians_dist[,3] < medians_dist[,7])/10000
#East_Hawaii
sum(medians_dist[,4] < medians_dist[,7])/10000
#East_Marquesas
sum(medians_dist[,5] < medians_dist[,7])/10000
#Simulated
sum(medians_dist[,6] < medians_dist[,7])/10000


medianCI<-as.data.frame(t(rbind(medians,CI95)))
colnames(medianCI)<-c("medians","low","high")
medianCI["barrier"]<-factor(rownames(medianCI), levels=c("West_Indian","West_Central","East_Central","East_Hawaii","East_Marquesas","Simulated","Randomized"))


phist_violins2<-phist_violins  + geom_pointrange(data=medianCI,mapping=aes(x=barrier,y=medians,ymin=low,ymax=high), inherit.aes=F,fatten=1) + coord_cartesian(ylim=c(0,0.1))

phist_medians<-ggplot(data=medianCI) + geom_pointrange(data=medianCI,mapping=aes(x=barrier,y=medians,ymin=low,ymax=high), inherit.aes=F,fatten=4) + scale_x_discrete(labels=c("1. Western Indian/ \n Western Indo-Pacific","2. Western Indo-Pacific / \n Central Indo-Pacific", "3. Central Indo-Pacific / \n Eastern Indo-Pacific", "4. Eastern Indo-Pacific / \n Hawaii", "5. Eastern Indo-Pacific / \n Marquesas","Simulated", "Randomized")) + ylab(expression(Phi[ST])) + xlab("Barrier")  + stat_summary(data=extracted_Fst_melt,mapping=aes(x=variable,y=value),fun.data = n_fun, geom = "text", alpha=1, position_fill(vjust=1.10))  + coord_cartesian(ylim=c(0,0.10)) + theme(axis.text.x = element_text(angle=45, hjust=1))
 
medianCI

phist_violins2

phist_medians

ggsave(phist_medians,file="./output/phist_medians5.pdf", width=7, height=7, units="in")
ggsave(phist_violins,file="./output/phist_violins5.pdf", width=7, height=7, units="in")

# Count the zeros!
zerocount<-function(x){
  length(which(x == 0)) / length(which(!is.na(x)))
}

apply(extracted_Fst2,FUN=zerocount, MARGIN=2)

zerocount(simPHIST)
```

