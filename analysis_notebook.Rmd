---
title: "DIPnet Population Structure Notebook"
output: 
  html_notebook:
    toc: yes
---

# Setup
```{r Setup, message=FALSE, warning=FALSE}
library(seqinr)
library(ape)  
library(pegas)
library(hierfstat)
library(mmod)
library(adegenet)
library(plyr)
library(strataG)
library(iNEXT)
library(gdm)
library(gdistance)
library(ecodist)
library(dplyr)
library(reshape2)
library(WriteXLS)
library(ggplot2)
library(knitr)
library(vegan)
library(gridExtra)
library(data.table)


source("config.R")
source("DIPnet_Stats_Functions.R")

rescale<-function(x){
  normalized<-(x-min(x))/(max(x)-min(x))
  return(normalized)
}

remove.dup.gsls<-function(amovatable){
  amovatable<-amovatable[-which(rownames(amovatable) %in% dups),]
}

splinegrab<-function(ispline){
  x<-as.data.frame(ispline$x)
  y<-as.data.frame(ispline$y)
  xy<-as.data.frame(cbind(x$matrix_1,y$matrix_1))
}

splinegrab2<-function(ispline){
  x<-as.data.frame(x=ispline$x)
  melted<-melt(ispline$y)
  xy<-as.data.frame(cbind(d=x$matrix_1,spline=as.character(melted$Var2),value=melted$value))
}

```

```{r Import Data}
######################################################################
# 1. Import the IPDB and Fst tables
ipdb<-read.table(ipdb_path,sep="\t",header=T,stringsAsFactors = F,quote="", na.strings=c("NA"," ","")) 


#read in geographical regionalizations from Treml
spatial<-read.table(spatial_path, header=T, sep="\t",stringsAsFactors = F, na.strings=c("NA"," ",""), quote="")

#read in geographical regionalizations from Beger
spatial2<-read.table(spatial2_path, header=T,sep="\t", stringsAsFactors = F, na.strings=c("NA"," ",""), quote="")

#read in overwater distances and lookup table from Treml
outMat <-read.table("~/github/IPDB/distances/IPDB_OW.dist.csv", header=TRUE, sep=",", row.names=c(1))
  
LookUp <-read.table("~/github/IPDB/distances/ID_Lookup.csv", header=TRUE, sep=",")

#read in ABGD groups
abgd<-read.table(abgd_path, header=T, sep="\t", stringsAsFactors = F)

#join spatial
ipdb<-join(ipdb,spatial, by = "IPDB_ID",type = "left")
ipdb<-join(ipdb,spatial2[,c(2,18:24)], by = "IPDB_ID", type = "left")

#join ABGD
ipdb<-join(ipdb,abgd[,c(1,3)], by = "IPDB_ID",type = "left")

# drop hybrids and divergent individuals
ipdb<-ipdb[ipdb$IPDB_ID %in% drops == FALSE, ] 



## remove anything not included in the ecoregions scheme (some dolphins, some COTS from Kingman and Madagascar(?), some A. nigros from Kiribati, som C. auriga from Fakareva, hammerheads from Western Australia, and West Africa, and some dolphins from the middle of the eastern tropical pacific

ipdb_ecoregions<-ipdb[-which(is.na(ipdb$ECOREGION)),]

## remove anything that doesn't occur in the 3 Indo-Pacific realms
ipdb_ip<-ipdb_ecoregions[which(ipdb_ecoregions$REALM %in% c("Central Indo-Pacific","Western Indo-Pacific","Eastern Indo-Pacific")),]

amova_ts_path<-"/Users/eric/google_drive/DIPnet_Gait_Lig_Bird/DIPnet_WG4_first_papers/statistics/By_Species/Hierarchical_structure"

amova_abgd_path<-"/Users/eric/google_drive/DIPnet_Gait_Lig_Bird/DIPnet_WG4_first_papers/statistics/By_ESU/Hierarchical_structure"

# Read in Veron barriers
barriers<-read.csv("VeronBarriers.csv",header=F,stringsAsFactors = F)

#duplicate GSLs that need to be dropped
dups<-c("Acanthaster_planciSIO_CR","Acanthaster_planciRedSea_CR","Acanthaster_planciNIO_CR","Acanthaster_planciPac_CR","Acanthurus_nigrofuscus_CO1","Cephalopholis_argus_CO1","Chaetodon_auriga_CO1","Halichoeres_claudia_CR","Halichoeres_hortulanus_CO1","Halichoeres_hortulanus_CYB","Halichoeres_trimaculatus_CR","Lutjanus_kasmira_CO1","Naso_hexacanthus_CR","Neoniphon_sammara_CO1","Pomacentrus_coelestis_CO1","Pygoplites_diacanthus_CYB", "Stenella_longirostris_CYB")

hypotheses<-c("Bowen","Keith","Kulbicki_r","Kulbicki_b","REALM","PROVINCE","ECOREGION", "VeronDivis")

# Remove dups
ipdb_ip<-ipdb_ip[!ipdb_ip$Genus_species_locus %in% c(dups,"Ctenochaetus_marginatus_A68"),]
```


# Introduction

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. I am trying it out as a way to reproducibly document my work on the DIPnet Population Structure Paper.

The overall workflow for this analysis is as follows:

  - Run AMOVA according to several different regionalizations
    - Briggs and Bowen 2013 - Fish Biogeography
    - Veron et al. 2015 - Coral Biogeography
    - Spalding et al. 2007 - 
        - Marine Ecoregions
        - Provinces
        - Realms
    - Kulbicki et al. 2013 - 
        - Regionalization based on all species, 
        - or just "reliable" species
    - Keith et al. 2013 - Faunal Breaks
  - Select the best model for each species according to the Bayesian Information Criterion
  - Analyze individual "barriers" as implied by the regionalization that best explains the distribution of variation in the most species by modeling pairwise genetic distances given geographic distance and each barrier (individually or together):
    - Generalized Dissimilarity Modeling (GDM)
    - Multiple Regression with Distance Matrices (MRDM)
    - Distance Based Redundancy Analysis (dbRDA)
  - Special considerations:
    - What is a species? Do the analysis using:
        - Traditional species boundaries
        - Automated Barcode Gap Discovery (ABGD) to account for cryptic species
    - Include sequence distance in AMOVA?
        - No: traditional Fst, specifically Weir and Cockerhams Theta
        - Yes: Phi-ST

# Run AMOVA loops

Given the special considerations, we need to run 4 different flavors of AMOVA:
  1. WCTheta and TradSpec
  2. WCTheta and ABGD
  3. PhiST and TradSpec
  4. PhiST and ABGD

Thus, we need to loop through all of the regionalizations above, running all 4 flavors of AMOVA.


```{r AMOVA Loops, eval=F}

## Loop through hypotheses, calculating AMOVA
hypotheses<-c("Bowen","Keith","Kulbicki_r","Kulbicki_b","REALM","PROVINCE","ECOREGION", "VeronDivis")
amova_list<-list()

for(h in hypotheses){
  hierstats<-hierarchical.structure.mtDNA.db(ipdb = ipdb_ip,level1 = "sample",level2=h,model="N",ABGD=F,nperm=1)
  amova_list[[h]]<-hierstats
}
  
#load(file.path(amova_ts_path,"PHIST_TradSpecies_raw_amova_output.Rdata"))
## Summarize AMOVA results
amovastats<-summarize_AMOVA(amova_list,hypotheses,specieslist=unique(ipdb$Genus_species_locus))

WriteXLS(amovastats,ExcelFileName=file.path(amova_ts_path,"PHIST_ts_rawN_table_amova_output.Rdata.xlsx"),row.names = T)
save(amova_list,file=file.path(amova_ts_path,"PHIST_ts_rawN_amova_output.Rdata"))
save(amovastats,file=file.path(amova_ts_path,"PHIST_ts_tableN_amova_output.Rdata"))
```


This takes a long time, so we are not running this code in the document, but loading in the results as we go through each consideration

# Visualize AMOVA results


## PHIST and Traditional Species Boundaries


### Measure Support
```{r Measure Support2}
load(file=file.path(amova_ts_path,"PHIST_ts_tableN_amova_output.Rdata"))
#remove duplicate gsls
amovastats<-lapply(amovastats,remove.dup.gsls)

# Measure support for each hypothesis

#Get the values for each hypothesis for a given criterion - here I use BIC - and rank them for #each species, then choose the "best" hypothesis for each species based on the criterion.

criterion<-"BIC"
# find the maximum number of species from any of the 8 hypotheses
maxlength<-max(sapply(amovastats,function(x) length(x[[1]]))) 
# create an empty data frame with row names from the hypothesis with the most values
crit_df<-data.frame(setNames(replicate(maxlength,numeric(0), simplify = F),nm=row.names(amovastats[["PROVINCE"]]))) 

#Loop through the hypotheses, pulling out the values for criterion,transpose it, and then merge these values into the dataframe from the previous hypothesis 
for(h in names(amovastats)){
  crit_df<-merge(crit_df,t(amovastats[[h]][criterion]),all=T,sort=F)
}
#get the hypothesis names in there
row.names(crit_df)<-names(amovastats)

#rank the hypotheses for each species
crit_rank<-as.data.frame(sapply(crit_df,rank,na.last="keep",ties.method="average"))
row.names(crit_rank)<-names(amovastats)

## remove gsls with more than 3 missing models
crit_rank<-crit_rank[, colSums(is.na(crit_rank)) < nrow(crit_rank)-4]  

## choose the best hypothesis or set of hypotheses for each species  
best_hypothesis<-sapply(crit_rank,function(x){row.names(crit_rank)[which(x==min(x,na.rm=T)) ]})

# Add phylum to each
best_hypothesis<-as.data.frame(unlist(best_hypothesis))
best_hypothesis$Genus_species_locus<-row.names(best_hypothesis)
ipdb_phylum<-ipdb_ip[,c("Genus_species_locus","phylum")]
ipdb_phylum<-unique(ipdb_phylum)

#some grepping to get the extra numbers off the ends of gsl names
best_hypothesis$Genus_species_locus<-gsub("(\\w+_\\w+_CR)", replacement="\\1R",best_hypothesis$Genus_species_locus, perl=T)

best_hypothesis$Genus_species_locus<-gsub("(\\w+_\\w+_\\w\\w\\w)\\w", replacement="\\1", best_hypothesis$Genus_species_locus, perl=T)

best_hypothesis$Genus_species_locus<-gsub("(\\w+_\\w+_CR)R", replacement="\\1", best_hypothesis$Genus_species_locus, perl=T)


best_hypothesis<-left_join(best_hypothesis, ipdb_phylum, by="Genus_species_locus")

names(best_hypothesis)[1]<-"Hypothesis"

                               
## Make a bar graph of best support for each hypothesis among species

barplot<-ggplot(data=best_hypothesis, aes(x=Hypothesis,fill=phylum )) + geom_bar(aes(y = (..count..)/sum(..count..))) +
   scale_x_discrete(limits=hypotheses, labels=c("Briggs and Bowen \n k = 5","Keith et al. \n k = 11", "Kulbicki et al. \n Regions k = 3", "Kulbicki et al. \n Provinces k = 10", "Spalding et al. \n Realms k = 3", "Spalding et al. \n Provinces k = 27" , "Spalding et al. \n Ecoregions k = 25", "Veron et al. \n k = 12")) +
  labs(x="Hypothesis",y="Proportion of Species") + theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.title=element_blank())

barplot

#ggsave("./output/PhiST_Bars_Final.pdf", barplot, width=7,height=7,units="in")
```


### Calculate relative probability from Johnson and Omland 2004
```{r Relative Probability Heatmap2, echo=F}
# lookup the minimum BIC value for each species (which.min works better here, because it returns only the first instance of the minimum value)
minBIC<-sapply(crit_df,function(x){x[which.min(x)]}) 

#J&O box 4 eqn 1. scale() seems to be the way to go here, using the minBIC as the centering vector
crit_df_deltaI<-scale(crit_df, center=minBIC, scale=F) 

#J&O box 4 eqn 4. numerator, plus make it a data frame
crit_df_deltaI_b<-as.data.frame(exp(-0.5*crit_df_deltaI)) 

#J&O box 4 eqn 4. denominator
crit_df_deltaI_sums<-sapply(crit_df_deltaI_b,sum,na.rm=T) 

# this time use the scale argument of scale to divide each column by the corresponding sum
crit_df_relative_prob<-scale(crit_df_deltaI_b,center=F,scale=crit_df_deltaI_sums) 

# Now zap all GSLs that test less than 5 hypotheses

naCount<-colSums(is.na(crit_df_relative_prob))

na4<-names(naCount)[which(naCount<4)]


crit_df_relative_prob_na4<-crit_df_relative_prob[,na4]
```

There are `r length(na4)` GSLs that test at least 5 hypotheses

### Heatmap
So now make a trimmed heatmap of just these.

```{r}
## Make a new heatmap of relative probability of each hypothesis

#melt for ggplot2
relprob<-melt(crit_df_relative_prob_na4)
colnames(relprob)<-c("Hypothesis","Species","Relative_Probability")


#make names
spnames<-sub("(\\w+)_(\\w+)_\\w+","\\1 \\2",na4, perl=T)
spnames<-sort(spnames)

#baseplot
rp<-ggplot(relprob,aes(y=Species,x=Hypothesis,fill=Relative_Probability))

#add geom_tile, turn the x-axis elements by 90 degrees, reverse the names on the y-axis, and use a diverging color scheme to highlight hypotheses with >50% rel prob.
rp<-rp+geom_tile()+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  scale_fill_gradient(low = "white",
                       high = "red", space = "rgb",
                       na.value = "grey50", guide = "colourbar") +
  scale_x_discrete(limits=hypotheses, labels=c("Briggs and Bowen \n k = 5","Keith et al. \n k = 11", "Kulbicki et al. \n Regions k = 3", "Kulbicki et al. \n Provinces k = 10", "Spalding et al. \n Realms k = 3", "Spalding et al. \n Provinces k = 27" , "Spalding et al. \n Ecoregions k = 25", "Veron et al. \n k = 12")) +
  scale_y_discrete(limits=rev(sort(levels(relprob$Species))), labels=rev(spnames))
rp

#ggsave("./output/PhiST_Heatmap_Final.pdf", rp)
```



## FST and Traditional Species Boundaries

Code is suppressed for the other 3 examples

### Measure Support
```{r Measure Support1, echo=F}
load(file=file.path(amova_ts_path,"FST_TradSpec_table_amova_output.Rdata"))
#remove duplicate gsls
amovastats<-lapply(amovastats,remove.dup.gsls)




# Measure support for each hypothesis

#Get the values for each hypothesis for a given criterion - here I use BIC - and rank them for #each species, then choose the "best" hypothesis for each species based on the criterion.

criterion<-"BIC"
# find the maximum number of species from any of the 8 hypotheses
maxlength<-max(sapply(amovastats,function(x) length(x[[1]]))) 
# create an empty data frame with row names from the hypothesis with the most values
crit_df<-data.frame(setNames(replicate(maxlength,numeric(0), simplify = F),nm=row.names(amovastats[["PROVINCE"]]))) 

#Loop through the hypotheses, pulling out the values for criterion,transpose it, and then merge these values into the dataframe from the previous hypothesis 
for(h in names(amovastats)){
  crit_df<-merge(crit_df,t(amovastats[[h]][criterion]),all=T,sort=F)
}
#get the hypothesis names in there
row.names(crit_df)<-names(amovastats)

#rank the hypotheses for each species
crit_rank<-as.data.frame(sapply(crit_df,rank,na.last="keep",ties.method="average"))
row.names(crit_rank)<-names(amovastats)

## remove gsls with more than 3 missing models
crit_rank<-crit_rank[, colSums(is.na(crit_rank)) < nrow(crit_rank)-4]  

## choose the best hypothesis or set of hypotheses for each species  
best_hypothesis<-sapply(crit_rank,function(x){row.names(crit_rank)[which(x==min(x,na.rm=T)) ]})


# Add phylum to each
best_hypothesis<-as.data.frame(unlist(best_hypothesis))
best_hypothesis$Genus_species_locus<-row.names(best_hypothesis)
ipdb_phylum<-ipdb_ip[,c("Genus_species_locus","phylum")]
ipdb_phylum<-unique(ipdb_phylum)

#some grepping to get the extra numbers off the ends of gsl names
best_hypothesis$Genus_species_locus<-gsub("(\\w+_\\w+_CR)", replacement="\\1R",best_hypothesis$Genus_species_locus, perl=T)

best_hypothesis$Genus_species_locus<-gsub("(\\w+_\\w+_\\w\\w\\w)\\w", replacement="\\1", best_hypothesis$Genus_species_locus, perl=T)

best_hypothesis$Genus_species_locus<-gsub("(\\w+_\\w+_CR)R", replacement="\\1", best_hypothesis$Genus_species_locus, perl=T)


best_hypothesis<-left_join(best_hypothesis, ipdb_phylum, by="Genus_species_locus")

names(best_hypothesis)[1]<-"Hypothesis"

                               
## Make a bar graph of best support for each hypothesis among species

barplot<-ggplot(data=best_hypothesis, aes(x=Hypothesis,fill=phylum )) + geom_bar(aes(y = (..count..)/sum(..count..))) +
   scale_x_discrete(limits=hypotheses, labels=c("Briggs and Bowen \n k = 5","Keith et al. \n k = 11", "Kulbicki et al. \n Regions k = 3", "Kulbicki et al. \n Provinces k = 10", "Spalding et al. \n Realms k = 3", "Spalding et al. \n Provinces k = 27" , "Spalding et al. \n Ecoregions k = 25", "Veron et al. \n k = 12")) +
  labs(x="Hypothesis",y="Proportion of Species") + theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.title=element_blank())

barplot

#ggsave("./output/FST_Bars_Final.pdf", barplot,width=7,height=7,units="in")
```

### Calculate relative probability from Johnson and Omland 2004 and visualize with a heatmap
```{r Relative Probability Heatmap1, fig.height=11, fig.width=8.5}
# lookup the minimum BIC value for each species (which.min works better here, because it returns only the first instance of the minimum value)
minBIC<-sapply(crit_df,function(x){x[which.min(x)]}) 

#J&O box 4 eqn 1. scale() seems to be the way to go here, using the minBIC as the centering vector
crit_df_deltaI<-scale(crit_df, center=minBIC, scale=F) 

#J&O box 4 eqn 4. numerator, plus make it a data frame
crit_df_deltaI_b<-as.data.frame(exp(-0.5*crit_df_deltaI)) 

#J&O box 4 eqn 4. denominator
crit_df_deltaI_sums<-sapply(crit_df_deltaI_b,sum,na.rm=T) 

# this time use the scale argument of scale to divide each column by the corresponding sum
crit_df_relative_prob<-scale(crit_df_deltaI_b,center=F,scale=crit_df_deltaI_sums) 

# Now zap all GSLs that test less than 5 hypotheses

naCount<-colSums(is.na(crit_df_relative_prob))

na4<-names(naCount)[which(naCount<4)]


crit_df_relative_prob_na4<-crit_df_relative_prob[,na4]

## Make a heatmap of relative probability of each hypothesis

#melt for ggplot2
relprob<-melt(crit_df_relative_prob_na4)
colnames(relprob)<-c("Hypothesis","Species","Relative_Probability")

#make names
spnames<-sub("(\\w+)_(\\w+)_\\w+","\\1 \\2",na4, perl=T)
spnames<-sort(spnames)

#baseplot
rp<-ggplot(relprob,aes(y=Species,x=Hypothesis,fill=Relative_Probability))

#add geom_tile, turn the x-axis elements by 90 degrees, reverse the names on the y-axis, and use a diverging color scheme to highlight hypotheses with >50% rel prob.
rp<-rp+geom_tile()+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  scale_fill_gradient(low = "white",
                       high = "red", space = "rgb",
                       na.value = "grey50", guide = "colourbar") +
  scale_x_discrete(limits=hypotheses, labels=c("Briggs and Bowen \n k = 5","Keith et al. \n k = 11", "Kulbicki et al. \n Regions k = 3", "Kulbicki et al. \n Provinces k = 10", "Spalding et al. \n Realms k = 3", "Spalding et al. \n Provinces k = 27" , "Spalding et al. \n Ecoregions k = 25", "Veron et al. \n k = 12")) +
  scale_y_discrete(limits=rev(sort(levels(relprob$Species))), labels=rev(spnames))
rp

ggsave("./output/FST_Heatmap_Final.pdf", rp, width=7, height=7, units = "in")
```


## FST and ABGD Boundaries

### Measure Support
```{r Measure Support3, echo=F}
load(file=file.path(amova_abgd_path,"FST_ABGD_table_amova_output.Rdata"))
#remove duplicate gsls
#amovastats<-lapply(amovastats,remove.dup.gsls)

# Measure support for each hypothesis

#Get the values for each hypothesis for a given criterion - here I use BIC - and rank them for #each species, then choose the "best" hypothesis for each species based on the criterion.
criterion<-"BIC"
# find the maximum number of species from any of the 8 hypotheses
maxlength<-max(sapply(amovastats,function(x) length(x[[1]]))) 
# create an empty data frame with row names from the hypothesis with the most values
crit_df<-data.frame(setNames(replicate(maxlength,numeric(0), simplify = F),nm=row.names(amovastats[["PROVINCE"]]))) 

#Loop through the hypotheses, pulling out the values for criterion,transpose it, and then merge these values into the dataframe from the previous hypothesis 
for(h in names(amovastats)){
  crit_df<-merge(crit_df,t(amovastats[[h]][criterion]),all=T,sort=F)
}
#get the hypothesis names in there
row.names(crit_df)<-names(amovastats)

#rank the hypotheses for each species
crit_rank<-as.data.frame(sapply(crit_df,rank,na.last="keep",ties.method="average"))
row.names(crit_rank)<-names(amovastats)

## remove gsls with more than 3 missing models
crit_rank<-crit_rank[, colSums(is.na(crit_rank)) < nrow(crit_rank)-4]  

## choose the best hypothesis or set of hypotheses for each species  
best_hypothesis<-sapply(crit_rank,function(x){row.names(crit_rank)[which(x==min(x,na.rm=T)) ]})


## Make a bar graph of best support for each hypothesis among species

barplot<-ggplot(data=as.data.frame(unlist(best_hypothesis)), aes(x=unlist(best_hypothesis) )) + geom_bar(aes(y = (..count..)/sum(..count..))) +
   scale_x_discrete(limits=hypotheses, labels=c("Briggs and Bowen \n k = 5","Keith et al. \n k = 11", "Kulbicki et al. \n Regions k = 3", "Kulbicki et al. \n Provinces k = 10", "Spalding et al. \n Realms k = 3", "Spalding et al. \n Provinces k = 27" , "Spalding et al. \n Ecoregions k = 25", "Veron et al. \n k = 12")) +
  labs(x="Hypothesis",y="Proportion of Species") + theme(axis.text.x = element_text(angle = 45, hjust = 1))

barplot










```


### Calculate relative probability from Johnson and Omland 2004 and visualize with a heatmap
```{r Relative Probability Heatmap3, echo=F}
# lookup the minimum BIC value for each species (which.min works better here, because it returns only the first instance of the minimum value)
minBIC<-sapply(crit_df,function(x){x[which.min(x)]}) 

#J&O box 4 eqn 1. scale() seems to be the way to go here, using the minBIC as the centering vector
crit_df_deltaI<-scale(crit_df, center=minBIC, scale=F) 

#J&O box 4 eqn 4. numerator, plus make it a data frame
crit_df_deltaI_b<-as.data.frame(exp(-0.5*crit_df_deltaI)) 

#J&O box 4 eqn 4. denominator
crit_df_deltaI_sums<-sapply(crit_df_deltaI_b,sum,na.rm=T) 

# this time use the scale argument of scale to divide each column by the corresponding sum
crit_df_relative_prob<-scale(crit_df_deltaI_b,center=F,scale=crit_df_deltaI_sums) 


## Make a heatmap of relative probability of each hypothesis

#melt for ggplot2
relprob<-melt(crit_df_relative_prob)
colnames(relprob)<-c("Hypothesis","Species","Relative_Probability")

#baseplot
rp<-ggplot(relprob,aes(y=Species,x=Hypothesis,fill=Relative_Probability))

#add geom_tile, turn the x-axis elements by 90 degrees, reverse the names on the y-axis, and use a diverging color scheme to highlight hypotheses with >50% rel prob.
rp<-rp+geom_tile()+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  ylim(rev(levels(relprob$Species)))+
  scale_fill_gradient2(low = "blue", mid = "white",
                       high = "red", midpoint = 0.5, space = "rgb",
                       na.value = "grey50", guide = "colourbar")
rp
```

## PHIST and ABGD Boundaries

### Measure Support
```{r Measure Support4, echo=F}
load(file=file.path(amova_abgd_path,"PHIST_ABGD_table_amova_output.Rdata"))
#remove duplicate gsls
#amovastats<-lapply(amovastats,remove.dup.gsls)

# Measure support for each hypothesis

#Get the values for each hypothesis for a given criterion - here I use BIC - and rank them for #each species, then choose the "best" hypothesis for each species based on the criterion.

criterion<-"BIC"
# find the maximum number of species from any of the 8 hypotheses
maxlength<-max(sapply(amovastats,function(x) length(x[[1]]))) 
# create an empty data frame with row names from the hypothesis with the most values
crit_df<-data.frame(setNames(replicate(maxlength,numeric(0), simplify = F),nm=row.names(amovastats[["PROVINCE"]]))) 

#Loop through the hypotheses, pulling out the values for criterion,transpose it, and then merge these values into the dataframe from the previous hypothesis 
for(h in names(amovastats)){
  crit_df<-merge(crit_df,t(amovastats[[h]][criterion]),all=T,sort=F)
}
#get the hypothesis names in there
row.names(crit_df)<-names(amovastats)

#rank the hypotheses for each species
crit_rank<-as.data.frame(sapply(crit_df,rank,na.last="keep",ties.method="average"))
row.names(crit_rank)<-names(amovastats)

## remove gsls with more than 3 missing models
crit_rank<-crit_rank[, colSums(is.na(crit_rank)) < nrow(crit_rank)-4]  

## choose the best hypothesis or set of hypotheses for each species  
best_hypothesis<-sapply(crit_rank,function(x){row.names(crit_rank)[which(x==min(x,na.rm=T)) ]})


## Make a bar graph of best support for each hypothesis among species

barplot<-ggplot(data=as.data.frame(unlist(best_hypothesis)), aes(x=unlist(best_hypothesis) )) + geom_bar(aes(y = (..count..)/sum(..count..))) +
   scale_x_discrete(limits=hypotheses, labels=c("Briggs and Bowen \n k = 5","Keith et al. \n k = 11", "Kulbicki et al. \n Regions k = 3", "Kulbicki et al. \n Provinces k = 10", "Spalding et al. \n Realms k = 3", "Spalding et al. \n Provinces k = 27" , "Spalding et al. \n Ecoregions k = 25", "Veron et al. \n k = 12")) +
  labs(x="Hypothesis",y="Proportion of Species") + theme(axis.text.x = element_text(angle = 45, hjust = 1))

barplot

```


### Calculate relative probability from Johnson and Omland 2004 and visualize with a heatmap
```{r Relative Probability Heatmap4, echo=F}
# lookup the minimum BIC value for each species (which.min works better here, because it returns only the first instance of the minimum value)
minBIC<-sapply(crit_df,function(x){x[which.min(x)]}) 

#J&O box 4 eqn 1. scale() seems to be the way to go here, using the minBIC as the centering vector
crit_df_deltaI<-scale(crit_df, center=minBIC, scale=F) 

#J&O box 4 eqn 4. numerator, plus make it a data frame
crit_df_deltaI_b<-as.data.frame(exp(-0.5*crit_df_deltaI)) 

#J&O box 4 eqn 4. denominator
crit_df_deltaI_sums<-sapply(crit_df_deltaI_b,sum,na.rm=T) 

# this time use the scale argument of scale to divide each column by the corresponding sum
crit_df_relative_prob<-scale(crit_df_deltaI_b,center=F,scale=crit_df_deltaI_sums) 


# Now zap all GSLs that test less than 5 hypotheses

naCount<-colSums(is.na(crit_df_relative_prob))

na4<-names(naCount)[which(naCount<4)]


crit_df_relative_prob_na4<-crit_df_relative_prob[,na4]

## Make a heatmap of relative probability of each hypothesis

#melt for ggplot2
relprob<-melt(crit_df_relative_prob_na4)
colnames(relprob)<-c("Hypothesis","Species","Relative_Probability")


#make names
#spnames<-sub("(\\w+)_(\\w+)_\\w+","\\1 \\2",na4, perl=T)
#spnames<-sort(spnames)

#baseplot
rp<-ggplot(relprob,aes(y=Species,x=Hypothesis,fill=Relative_Probability))

#add geom_tile, turn the x-axis elements by 90 degrees, reverse the names on the y-axis, and use a diverging color scheme to highlight hypotheses with >50% rel prob.
rp<-rp+geom_tile()+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  scale_fill_gradient2(low = "blue", mid = "white",
                       high = "red", midpoint = 0.5, space = "rgb",
                       na.value = "grey50", guide = "colourbar") +
  scale_x_discrete(limits=hypotheses, labels=c("Briggs and Bowen \n k = 5","Keith et al. \n k = 11", "Kulbicki et al. \n Regions k = 3", "Kulbicki et al. \n Provinces k = 10", "Spalding et al. \n Realms k = 3", "Spalding et al. \n Provinces k = 27" , "Spalding et al. \n Ecoregions k = 25", "Veron et al. \n k = 12")) +
  scale_y_discrete(limits=rev(sort(levels(relprob$Species))))
rp
```


# Calculate Pairwise Differentiation Stats

By sample

```{r, eval=F}
diffstats<-pairwise.structure.mtDNA.db(ipdb=ipdb, gdist = "WC Theta", minseqs = 5, minsamps = 3, mintotalseqs = 0, nrep = 0, num.cores = 2, ABGD = T, regionalization = "sample")


save(diffstats, file="/Users/eric/google_drive/DIPnet_Gait_Lig_Bird/DIPnet_WG4_first_papers/statistics/By_ESU/Pairwise_statistics/DIPnet_structure_sample_WCTheta_ABGD.Rdata")

diffstats<-pairwise.structure.mtDNA.db(ipdb=ipdb, gdist = "Jost D", minseqs = 5, minsamps = 3, mintotalseqs = 0, nrep = 0, num.cores = 2, ABGD = T, regionalization = "sample")


save(diffstats, file="/Users/eric/google_drive/DIPnet_Gait_Lig_Bird/DIPnet_WG4_first_papers/statistics/By_ESU/Pairwise_statistics/DIPnet_structure_sample_JostD_ABGD.Rdata")
```



# Mixing Bowen and Spalding Realms
First step is to distinguish Hawaii, Easter Island and the Marquesas, thereby mixing in a little bit of Briggs and Bowen


```{r}
unique(ipdb_ip$Bowen)

unique(ipdb_ip$REALM)

dim(ipdb_ip)


ipdb_ip$REALM[which(ipdb_ip$Bowen=="Hawaiian")]<-"Hawaii"
ipdb_ip$REALM[which(ipdb_ip$Bowen=="Marquesas")]<-"Marquesas"
ipdb_ip$REALM[which(ipdb_ip$Bowen=="Easter")]<-"Easter"

unique(ipdb_ip$REALM)


```


# Generalized Dissimilarity Modeling

## GDM code
```{r GDM1, message=FALSE, warning=FALSE}

  load("/Users/eric/google_drive/DIPnet_Gait_Lig_Bird/DIPnet_WG4_first_papers/statistics/By_Species/Pairwise_statistics/sample/DIPnet_structure_sample_PhiST.Rdata")

#esu_loci <- unique(ipdb$Genus_species_locus)
#esu_loci <- esu_loci[-which(esu_loci %in% dups)]

esu_loci<-na4

esu_loci<-sort(esu_loci)

gdm.full<-list()

solution<-list()
nosolution<-list()
nosolution.full<-list()
nosolution.list<-list()

stats.full<-data.frame(Species_Locus=character(0),FullDeviance=numeric(0),FullExplainedDeviance=numeric(0),DistDeviance=numeric(0),FullExplainedDeviance=numeric(0),DeltaDeviance=numeric(0),region_deviance_explained=numeric(0), dist_deviance_explained=numeric(0), both_deviance_explained=numeric(0),unexplained_deviance=numeric(0), stringsAsFactors = F)

stats<-data.frame(Species_Locus=character(0),Barrier=character(0),WithBarrierDeviance=numeric(0),WithBarrierExplainedDeviance=numeric(0),ImportanceDistanceWithBarrier=numeric(0),ImportanceBarrierWithBarrier=numeric(0),NoBarrierDeviance=numeric(0),NoBarrierExplainedDeviance=numeric(0),ImportanceDistanceWithoutBarrier=numeric(0),DeltaDeviance=numeric(0),Pvalue_barrier=numeric(0),Pvalue_dist=numeric(0),MRDM.rsquared=numeric(0),MRDM.rsquared.pvalue=numeric(0),MRDM.dist.pvalue=numeric(0), MRDM.barrier.pvalue=numeric(0),stringsAsFactors = F)

nostats<-NULL

barriertests<-data.frame(Species=character(0),Region1=character(0),NumPops1=numeric(0),Region2=character(0),Numpops2=numeric(0), Test=logical(0), Solution=logical(0),stringsAsFactors = F)

plotlist<-list()

k<-0

for(gsl in esu_loci){ #gsl<-"Linckia_laevigata_CO1" "Tridacna_crocea_CO1" "Lutjanus_kasmira_CYB" "Acanthaster_planci_CO1"
  
  cat("\n","\n","\n","Now starting", gsl, "\n")
  
# skipping this because we are deleting individual rows/columns with NAs  if(any(is.na(diffstats[[gsl]]))){cat("NAs in FST table, No gdm calculated"); nostats<-c(nostats,paste(gsl,"NAs in FST"));next}
  
  if(diffstats[[gsl]]=="Fewer than 3 sampled populations after filtering. No stats calculated"){nostats<-c(nostats,paste(gsl,"N<3 NO FST"));next}
  
  #pull out the data for this genus-species-locus (gsl)
  sp<-ipdb[which(ipdb$Genus_species_locus==gsl),]
  #clean weird backslashes from names
  sp$locality<-gsub("\"","",sp$locality)
  
  sp$sample<-paste(sp$locality,round(sp$decimalLatitude, digits=0),round(sp$decimalLongitude, digits=0),sep="_")  #sets up a variable that matches the name in Fst table
  sp<-sp[order(sp$sample),]
  # Not all localities are included in Veron's regionalization (e.g. Guam), so get their names and then zap NAs
  nonVeronpops<-unique(sp$sample[is.na(sp$VeronDivis)])
  sp<-sp[!is.na(sp$VeronDivis),]
  
  #subsample Fst 
  gslFST<-diffstats[[gsl]]
  #make a matrix out of gslFST
  gslFSTm<-as.matrix(gslFST)
  
  #remove rows and columns with NAs
  gslFSTm<-gslFSTm[complete.cases(gslFSTm),complete.cases(gslFSTm)]
  
  gslFSTm[which(gslFSTm > 1)] <- 1 #some values that look like 1.0000 are registering as greater than 1
  gslFSTm[which(gslFSTm < 0)] <- 0.00001 #get rid of artifactual negative values
  #gslFSTm<-rescale(gslFSTm)
  #gslFSTm<-gslFSTm/(1-gslFSTm)
  
  #zap weird slashes in the names
  rownames(gslFSTm)<-gsub("\"","",rownames(gslFSTm))
  colnames(gslFSTm)<-rownames(gslFSTm)
  
  #zap the same na populations from the list of non existent pops from VeronDivis
  if(any(rownames(gslFSTm) %in% nonVeronpops)){
    gslFSTm<-gslFSTm[-(which(rownames(gslFSTm) %in% nonVeronpops)),-(which(colnames(gslFSTm) %in% nonVeronpops))]
  }
  
#  if(length(rownames(gslFSTm))<5){nostats<-c(nostats,paste(gsl,"pops<5"));cat("Fewer than 5 sampled populations");next}
  
  #and filter sp based on the localities that have Fst values
  sp<-sp[sp$sample %in% rownames(gslFSTm),]
  
  #and vice versa
  
  gslFSTm<- gslFSTm[which(rownames(gslFSTm) %in% unique(sp$sample)),which(rownames(gslFSTm) %in% unique(sp$sample))]
  


  #create a locations data frame that has all the localities plus lats and longs and their Veron region.
  locs<-as.data.frame(unique(sp$sample))
  names(locs)<-"sample"
  #locs$Long<-sp$decimalLongitude[which(locs %in% sp$sample)]
  #can't do a unique on sample, lats and longs because some samples have non-unique lats and longs! So I do a join and take the first match.
  locs<-join(locs,sp[c("IPDB_index","sample","decimalLongitude",
                       "decimalLatitude","Bowen","Kulbicki_r","REALM")],
                        by="sample", match="first")
  
  
  #sort gslFSTm
  gslFSTm<-gslFSTm[order(rownames(gslFSTm)),order(colnames(gslFSTm))]
  # convert to data frame with popsample names as first column
  gslFSTm<-cbind(sample=locs$sample,as.data.frame(gslFSTm))
  
  ######################################################################
  # 3. Calculate Great Circle Distance
  gcdist_km <- pointDistance(locs[,3:4],lonlat=T)/1000
  #symmetricize the matrix
  gcdist_km[upper.tri(gcdist_km)]<-t(gcdist_km)[upper.tri(gcdist_km)]
  
  #cbind on the sample names
  gcdist_km <- cbind(sample=locs$sample,as.data.frame(gcdist_km))
  
  ######################################################################
  # 4. Lookup Overwater Distances - Code and Distances from Eric Treml and his RA Jutta
  mySites<-locs$IPDB_index
  
  SubSet.Sites<-setDT(LookUp, key = 'IPDB_index')[list(mySites)]

  myIndex<-SubSet.Sites$Rid
  
  # some distances were not measurable due to point being 20km from water
  # zap these points from both matrices
  #rownames(mySubOutMat)<-mySites
  #colnames(mySubOutMat)<-mySites
  if(anyNA(myIndex)){
  nazaps<-which(is.na(myIndex))
  myIndex<-myIndex[-nazaps]
  gcdist_km<-gcdist_km[-nazaps,-(nazaps+1)]
  gslFSTm<-gslFSTm[-nazaps,-(nazaps+1)]
  locs<-locs[-nazaps,]
 
  }
  
  owdist_km<-outMat[myIndex,myIndex]
  
  #cbind on the sample names
  owdist_km <- cbind(sample=locs$sample,as.data.frame(owdist_km))
  
 
  
  ####################################################################
  #5. Format the data and run in full, distance and region models
  
  cat("Running Full Model")
  
  
  regions<-cbind(locs[,c(1,3,4)],REALM=rep_len(1,length(locs[,1])))
  if(length(unique(locs$REALM)) > 1){
    regions<-cbind(locs[,c(1,3,4)],with(locs, data.frame(model.matrix(~REALM+0))))
  }
  
  gslFSTmll<-cbind(locs[,c(3,4)],gslFSTm)
  
  gdm.format.full<-formatsitepair(bioData=gslFSTm, bioFormat=3, predData=regions,XColumn = "decimalLongitude", YColumn = "decimalLatitude", siteColumn="sample", distPreds=list(owdist_km))
  
     #zap population pairs with 0 geographical distance between them. Troubling.
    zaps<-which(gdm.format.full$s2.matrix_1==0)
    
    if(length(zaps)>0) {
    gdm.format.full<-gdm.format.full[-zaps,]
    }
  
  # run the full model, and the model with only distance, and the model with only regions
  fullgdm<-gdm(gdm.format.full)
  distgdm<-gdm(gdm.format.full[,c(1:6,grep("matrix_1",names(gdm.format.full)))])
  regiongdm<-gdm(gdm.format.full[,c(1:6,grep("REALM",names(gdm.format.full)))])
  
  if(is.null(distgdm)){
      cat("Distance GDM no solution!")
      distgdm<-list(gdmdeviance=fullgdm$nulldeviance,explained=0)
  }
  
  if(is.null(regiongdm)){
    cat("Region GDM no solution!")
    regiongdm<-list(gdmdeviance=fullgdm$nulldeviance,explained=0)}
  

#6.  plot each species
  
  # if fullgdm is not null plot the ispline and linear model
if(!is.null(fullgdm)){
  ibd<-as.data.frame(cbind(distance=gdm.format.full$s2.matrix_1, phist=gdm.format.full$distance))
  
  #extract the ispline

  ispline<-isplineExtract(fullgdm)
  ispline2<-splinegrab(ispline)

    #plot both the ispline (green) and the linear model (purple)
ibdplot<- ggplot(data=ibd,mapping=aes(x=distance,y=phist)) + geom_point() +
      geom_smooth(method=lm, color="purple") +
      geom_line(data=ispline2,mapping=aes(x=V1,y=V2), color="green") +
      ggtitle(label=gsl)

plotlist[[gsl]]<-ibdplot
}

  

if(!(is.null(distgdm) | is.null(distgdm$gdmdeviance) | distgdm$explained==0)){
  ibd<-as.data.frame(cbind(distance=gdm.format.full$s2.matrix_1, phist=gdm.format.full$distance))
  
  #extract the ispline

  ispline<-isplineExtract(distgdm)
  ispline3<-splinegrab(ispline)

    #plot the ispline
ibdplot<-ibdplot + geom_line(data=ispline3,mapping=aes(x=V1,y=V2), color="blue")
    

plotlist[[gsl]]<-ibdplot
}

  
  #if fullgdm is null, then still plot the linear model  
if(is.null(fullgdm)){
  ibd<-as.data.frame(cbind(distance=gdm.format.full$s2.matrix_1, phist=gdm.format.full$distance))
  
  ibdplot<- ggplot(data=ibd,mapping=aes(x=distance,y=phist)) +geom_point()+
    geom_smooth(method=lm, color="purple") + ggtitle(gsl)
  plotlist[[gsl]]<-ibdplot
}

#7. calculate the relative contributions to deviance explained for geographical distance and regions and both
  
#plot(gdm.format.full$s2.matrix_1,gdm.format.full$distance, main=gsl)

# Code from Cynthia Riginos 9/10/17
#Following Borcard et al. 1992 
#a = regions
#b = regions & dist
#c = distance
#d = unexplained variance

if(is.null(fullgdm)){
  fullgdm<-list(gdmdeviance=0.00001,explained=0.00001,nulldeviance=0.00001)
  if(is.null(regiongdm$gdmdeviance)){
    regiongdm<-list(gdmdeviance=0.00001,explained=0.00001,nulldeviance=0.00001)}
  if(is.null(distgdm$gdmdeviance)){
    distgdm<-list(gdmdeviance=0.00001,explained=0.00001,nulldeviance=0.00001)}
}
  
ABC<-(fullgdm$nulldeviance-fullgdm$gdmdeviance)/fullgdm$nulldeviance
AB<-(fullgdm$nulldeviance-regiongdm$gdmdeviance)/fullgdm$nulldeviance
BC<-(fullgdm$nulldeviance-distgdm$gdmdeviance)/fullgdm$nulldeviance

A<-ABC-BC
C<-ABC-AB
B<-ABC-A-C
if(B<0){B<-0; A<-0}
D<- 1-ABC


if(fullgdm$nulldeviance==0.00001){
        cat("No solution obtained on Full Model")
  
   if(regiongdm$nulldeviance==0.00001 & distgdm$nulldeviance==0.00001){
     A<-0;B<-0;C<-0;D<-1
   }
   
   if(regiongdm$nulldeviance==0.00001 & distgdm$nulldeviance!=0.00001){
          A<-0
          B<-0
          C<-(distgdm$nulldeviance-distgdm$gdmdeviance)/distgdm$nulldeviance
          D<-1-C
          if(distgdm$explained<0){A<-0;B<-0;C<-0;D<-1}
   }
   if(regiongdm$nulldeviance!=0.00001 & distgdm$nulldeviance==0.00001){
          
          A<-(regiongdm$nulldeviance-regiongdm$gdmdeviance)/regiongdm$nulldeviance
          B<-0
          C<-0
          D<-1-A}
   }
     
  
  # pull out stats
   #difference in deviance
    deltadev.regions<-distgdm$gdmdeviance-fullgdm$gdmdeviance
    
    #percent of null deviance explained by the model with just distance
    explaineddev.dist<-distgdm$explained
    
    gdm.full.deviance<-fullgdm$gdmdeviance
    gdm.full.explained<-fullgdm$explained

    gdm.dist.deviance<-distgdm$gdmdeviance
    gdm.dist.explained<-distgdm$explained
  
  # ##############################################################################
  #   # 4A. Perform Monte-Carlo permutations to develop a null distribution 
  #   #    of deviance values and determine significance (pvalue)
  #   gdm.format.rand.full<-gdm.format.full
  #   rand.deltas.full<-vector() 
  #   rand.explained.full<-vector()
  # 
  #   while(length(rand.deltas.full) < 1000) {
  #     gdm.format.rand.full$distance<-sample(gdm.format.rand.full$distance,size=length(gdm.format.rand.full$distance))
  #     gdm.barrier.rand.full<-gdm(gdm.format.rand.full)
  #     gdm.no.barrier.rand.full<-gdm(gdm.format.rand.full[,c(1:6,grep("matrix_1",names(gdm.format.rand.full)))])
  #   
  #       # if no solution obtained, go to next gsl
  #     if(is.null(gdm.barrier.rand.full) | is.null(gdm.no.barrier.rand.full)){next}
  #     
  #     deltadev.rand.full<-gdm.no.barrier.rand.full$gdmdeviance-gdm.barrier.rand.full$gdmdeviance
  #     explained.rand.full<-gdm.no.barrier.rand.full$explained
  #     
  #     rand.deltas.full<-c(rand.deltas.full,deltadev.rand.full)
  #     rand.explained.full<-c(rand.explained.full,explained.rand.full)
  #   }
  #   pvalue_regions.full<-length(which(abs(deltadev.regions) < abs(rand.deltas.full)))/length(rand.deltas.full)
  #   pvalue_dist.full<-length(which(abs(explaineddev.dist) < abs(rand.explained.full)))/length(rand.explained.full)
  #   
    stats.1<-c(gsl,gdm.full.deviance,gdm.full.explained,gdm.dist.deviance,gdm.dist.explained,deltadev.regions, A, C, B, D)

    
    stats.full[nrow(stats.full)+1,]<-stats.1
    
    gdm.full[[gsl]]<-fullgdm
    
    
}

ibdplots<-marrangeGrob(plotlist, nrow=2,ncol=2)
ggsave("./output/ibdplots_test.pdf",ibdplots)
  ####################################################################### Calculate Overwater Distances#
  #Save for later##
  #######################################################################
  # 5. Create a subset of the distance matrices including only the localities from
  #    two neighboring Veron regions.
  
  #make a table to keep track of all these tests for each species
  
# individual barrier tests
  # for(j in 1:16){
  #   k<-k+1
  #   barrier<-c(barriers[j,1],barriers[j,2])
  #   subset_locs<-which(locs$VeronDivis==barrier[1] | locs$VeronDivis==barrier[2])
  #   locs2<-locs[subset_locs,]
  #   
  #   
  #   Numpops1<-length(locs2$sample[which(locs$VeronDivis==barrier[1])])
  #   Numpops2<-length(locs2$sample[which(locs$VeronDivis==barrier[2])])
  #   
  #   cat("Now Starting",barrier,"\n")
  #   
  #   gcdist_km2<-gcdist_km[subset_locs,c(1,subset_locs+1)]
  #   gslFSTm2<-gslFSTm[subset_locs,c(1,subset_locs+1)]
  #   
  #   #######################################################################
  #   # 6. Create a dummy distance matrix for each putative "barrier" 
  #   #     between the two regions (1s and 0s)
  #   
  #   barrier_m2<-as.matrix(dist(as.numeric(locs2$VeronDivis %in% barrier[1])))
  #   
  #   barrier_m2 <- cbind(sample=locs2$sample,as.data.frame(barrier_m2))
  #   
  #   #if there aren't enough samples on either side of this barrier, then record this as non testable and go to next barrier
  #   if(Numpops1+Numpops2 < 3){cat("Less than three sampled populations; not testable\n"); barriertests[k,]<-c(gsl,barrier[1],Numpops1,barrier[2],Numpops2,F,F);next}
  #   if( Numpops1 < 1) {cat("not enough populations within",barrier[1],"\n") ; barriertests[k,]<-c(gsl,barrier[1],Numpops1,barrier[2],Numpops2,F,F);next}
  #   if( Numpops2 < 1) {cat("not enough populations within",barrier[2],"\n") ; barriertests[k,]<-c(gsl,barrier[1],Numpops1,barrier[2],Numpops2,F,F);next}
  #   
  #   ############################################################################
  #   # 7. Run through gdm with the barrier and without. Save the deviance values.
  #   
  #   locs2$sample<-as.character(locs2$sample)
  #   gslFSTm2$sample<-as.character(gslFSTm2$sample)
  #   gcdist_km2$sample<-as.character(gcdist_km2$sample)
  #   
  #   gdm.format<-formatsitepair(bioData=gslFSTm2, bioFormat=3, predData=locs2[,1:3],XColumn = "decimalLongitude", YColumn = "decimalLatitude", siteColumn="sample", distPreds=list(gcdist_km2,barrier_m2))
  #   
  #    #zap population pairs with 0 geographical distance between them. Troubling.
  #   zaps<-which(gdm.format$s2.matrix_1==0)
  #   
  #   if(length(zaps)>0) {
  #   gdm.format<-gdm.format[-zaps,]
  #   }
  #   
  #   #run gdm with and without the barrier
  #   gdm.barrier<-gdm(gdm.format)
  #   gdm.no.barrier<-gdm(gdm.format[,-grep("matrix_2",names(gdm.format))])
  #   
  #   # run mrdm
  #   mrdm<-MRM(formula = distance ~ s2.matrix_1 + s2.matrix_2, data=gdm.format,nperm = 10000)
  #   
  #   #TROUBLESHOOTING: save fst matrices from gdm models that obtain no solution
  #   if(is.null(gdm.barrier) | is.null(gdm.no.barrier))
  #     {cat("No Solution Obtained \n");
  #     nosolution[[paste(gsl,barrier[1],Numpops1,barrier[2],Numpops2,sep=",")]]<-list(locs2,gcdist_km2,gslFSTm2,gdm.format);
  #     barriertests[k,]<-c(gsl,barrier[1],Numpops1,barrier[2],Numpops2,T,F);
  #     next}
  #   
  #   #difference in deviance
  #   deltadev<-gdm.no.barrier$gdmdeviance-gdm.barrier$gdmdeviance
  #   
  #   #percent of null deviance explained by the model with just distance
  #   explaineddev<-gdm.no.barrier$explained
  #   
  #   ##############################################################################
  #   # 8. Perform Monte-Carlo permutations to develop a null distribution 
  #   #    of deviance values and determine significance (pvalue)
  #   gdm.format.rand<-gdm.format
  #   rand.deltas<-vector() 
  #   rand.explained<-vector()
  #   
  #   while(length(rand.deltas) < 1000) {
  #     gdm.format.rand$distance<-sample(gdm.format.rand$distance,size=length(gdm.format.rand$distance))
  #     gdm.barrier.rand<-gdm(gdm.format.rand)
  #     gdm.no.barrier.rand<-gdm(gdm.format.rand[,-grep("matrix_2",
  #                                                     names(gdm.format))])
  #     if(is.null(gdm.barrier.rand) | is.null(gdm.no.barrier.rand)){next}
  #     deltadev.rand<-gdm.no.barrier.rand$gdmdeviance-gdm.barrier.rand$gdmdeviance
  #     explained.rand<-gdm.no.barrier.rand$explained
  #     
  #     rand.deltas<-c(rand.deltas,deltadev.rand)
  #     rand.explained<-c(rand.explained,explained.rand)
  #   }
    # pvalue_barrier<-length(which(abs(deltadev) < abs(rand.deltas)))/length(rand.deltas)
    # pvalue_dist<-length(which(abs(explaineddev) < abs(rand.explained)))/length(rand.explained)
  #   
  #   cat("Good Solution \n")
  #   
  #   
  #   #save stats
  #   gdm.barrier.deviance<-gdm.barrier$gdmdeviance
  #   gdm.barrier.explained<-gdm.barrier$explained
  # 
  #   gdm.no.barrier.deviance<-gdm.no.barrier$gdmdeviance
  #   gdm.no.barrier.explained<-gdm.no.barrier$explained
  # 
  #   impt.dist.gdm.barrier<-sum(gdm.barrier$coefficients[1:gdm.barrier$splines[1]])
  #   impt.barrier.gdm.barrier<-sum(gdm.barrier$coefficients[gdm.barrier$splines[1]+1:gdm.barrier$splines[1]])
  #   impt.dist.gdm.no.barrier<-sum(gdm.no.barrier$coefficients[1:gdm.no.barrier$splines[1]])
  #   
  #   mrdm.rsquared<-mrdm$r.squared[1]
  #   mrdm.rsquared.pvalue<-mrdm$r.squared[2]
  #   mrdm.dist.pvalue<-mrdm$coef[2,2]
  #   mrdm.barrier.pvalue<-mrdm$coef[3,2]
  #   
  #   
  #  
  #   stats_model<-c(gsl,paste(barrier[1],barrier[2],sep="-"),gdm.barrier.deviance,gdm.barrier.explained, impt.dist.gdm.barrier, impt.barrier.gdm.barrier, gdm.no.barrier.deviance, gdm.no.barrier.explained,impt.dist.gdm.no.barrier, deltadev,pvalue_barrier,pvalue_dist,mrdm.rsquared,mrdm.rsquared.pvalue,mrdm.dist.pvalue,mrdm.barrier.pvalue)
  #   
  #   stats[nrow(stats)+1,]<-stats_model
  #   
  #   #record this in the table
  #   barriertests[k,]<-c(gsl,barrier[1],Numpops1,barrier[2],Numpops2,T,T)
  #   
  # }
  




#stats.full$GDM.regions.qvalue<-p.adjust(as.numeric(stats.full$Pvalue_regions),method="fdr")
#stats.full$GDM.distance.qvalue<-p.adjust(as.numeric(stats.full$Pvalue_dist),method="fdr")

# stats$GDM.qvalue<-p.adjust(as.numeric(stats$Pvalue_barrier),method="fdr")
# stats$MRDM.r2.qvalue<-p.adjust(as.numeric(stats$MRDM.rsquared.pvalue),method="fdr")
# stats$MRDM.dist.qvalue<-p.adjust(as.numeric(stats$MRDM.dist.pvalue),method="fdr")
# stats$MRDM.barrier.qvalue<-p.adjust(as.numeric(stats$MRDM.barrier.pvalue),method="fdr")



# # do some figuring with the results
# length(barriertests[which(barriertests$Test==T),1])
# #working GDM tests
# length(barriertests[which(barriertests$Solution==T & barriertests$Test==T),1])
# #failed GDM tests
# length(barriertests[which(barriertests$Solution==F & barriertests$Test==T),1])
# 
# 
# 
# length(stats[which(stats$GDM.qvalue <= 0.02),1])
# 
# length(stats[which(stats$MRDM.barrier.qvalue <= 0.02),1])
# 
# #overlap between MRDM and GDM
# length(stats[which(stats$GDM.qvalue <= 0.02 & stats$MRDM.barrier.qvalue <= 0.02),1])
# 
# 
# 
# 
# 
# goodbarriers<-stats %>% filter(GDM.qvalue < 0.02) %>% group_by(Barrier) %>% summarize(goodbarriers = n())
# 
# allbarriers<-stats %>% group_by(Barrier) %>% summarize(barrier_tests = n())
# 
# barrier_ratios<-left_join(allbarriers,goodbarriers,by="Barrier")
# 
# barrier_ratios$goodbarriers[which(is.na(barrier_ratios$goodbarriers))]<-0
# 
# barrier_ratios<-mutate(barrier_ratios, goodbarriers/barrier_tests)
# 
# 
# kable(barrier_ratios)
# 
# write.csv(stats,"./output/GDM_output_WCTheta_1000reps.csv")
# write.csv(barrier_ratios, "./output/GDM_JostD_WCTheta_barriers.csv")
```

## Create a heatmap from GDM regions
```{r GDM Heatmap, eval=F}

#extract the isplines
isplines<-lapply(gdm.full,isplineExtract)

#create a function that takes the last row of each ispline = importance, and lapply it
gdm.importance<-function(p){
  p$y[200,]
}
importance<-lapply(isplines,gdm.importance)

# create a function that transposes and converts to a data frame, and lapply it
dataframer<-function(p){
  q<-as.data.frame(t(p))
}

importance2<-lapply(importance,dataframer)

#merge the list of dataframes as found here: https://stackoverflow.com/questions/8091303/simultaneously-merge-multiple-data-frames-in-a-list
importance_df<-Reduce(function(t1,t2) merge(t1,t2,all=T,sort=F), importance2)

#same for the no solution set
nosolution_df<-Reduce(function(t1,t2) merge(t1,t2,all=T,sort=F), nosolution.full)

#importance_df<-importance_df[order(stats.full$GDM.regions.qvalue),]
#stats.full<-stats.full[order(stats.full$GDM.regions.qvalue),]

# convert to matrix
importance_matrix<-as.matrix(importance_df)
# split into regions component
regions_matrix<-importance_matrix[,2:dim(importance_matrix)[2]]
# and distance component
distance_matrix<-importance_matrix[,1]

# set importance values for models that with non-significant regions component at q<0.01 to zero
regions_matrix[which(stats.full$GDM.regions.qvalue > 0.01),][!is.na(regions_matrix[which(stats.full$GDM.regions.qvalue > 0.01),])]<-0

# set importance values for models with non-significant distance component at q<0.01 to zero
distance_matrix[which(stats.full$GDM.distance.qvalue > 0.01)]<-0

# stick the matrix back together and then scale importance to relative values based on max importance

importance_matrix2<-cbind(distance_matrix,regions_matrix)

importance_matrix3<-importance_matrix2/max(importance_matrix2,na.rm = T)

importance_df2<-as.data.frame(importance_matrix3,row.names = stats.full$Species_Locus)

importance_df2$Species<-names(gdm.full)

#merge in the nosolutions
names(nosolution_df)[1]<-"distance_matrix"
importance_df2<-merge(importance_df2, nosolution_df,all=T)

#delete the species that only occurred in one region, then delete that column
#importance_df2<-importance_df2[which(is.na(importance_df2$VeronDivis)),]
importance_df2<-importance_df2[,-which(names(importance_df2)=="REALM")]


write.csv(importance_df2,"./output/gdm_phist_importance.csv")


## Make a heatmap of the importance values

#melt for ggplot2
importance_melted<-melt(importance_df2,id.vars = "Species")
colnames(importance_melted)<-c("Species","Variable","Importance")
importance_melted$Species<-as.factor(importance_melted$Species)

#baseplot
ip<-ggplot(importance_melted,aes(y=Species,x=Variable,fill=Importance))

#add geom_tile, turn the x-axis elements by 90 degrees, reverse the names on the y-axis, and use a diverging color scheme to highlight hypotheses with >50% rel prob.
ip<-ip+geom_tile()+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+
  ylim(rev(levels(importance_melted$Species)))+
  scale_fill_gradientn(colors=c("white","pink","red"),values = c(0,0.000001,1),
                       na.value = "grey50", guide = "colourbar")
ip


```

## Plot stacked bars 
of deviance explained by distance, regions, distance+regions, unexplained
```{r}
stats.bars<-stats.full[,c("Species_Locus","region_deviance_explained","dist_deviance_explained","both_deviance_explained","unexplained_deviance")]

#temp until I figure out what is going on with this one
#stats.bars<-stats.bars[-which(stats.bars$Species_Locus=="Cellana_sandwicensis_CO1"),]

stats.bars$region_deviance_explained<-as.numeric(stats.bars$region_deviance_explained)
stats.bars$dist_deviance_explained<-as.numeric(stats.bars$dist_deviance_explained)
stats.bars$both_deviance_explained<-as.numeric(stats.bars$both_deviance_explained)
stats.bars$unexplained_deviance<-as.numeric(stats.bars$unexplained_deviance)

bars.sum<-stats.bars$region_deviance_explained+stats.bars$dist_deviance_explained+stats.bars$both_deviance_explained+stats.bars$unexplained_deviance


bars.melt<-melt(stats.bars, id.vars="Species_Locus")
bars.melt$variable<-factor(bars.melt$variable, levels=rev(levels(bars.melt$variable)))
bars.melt$Species_Locus<-factor(bars.melt$Species_Locus)
bars.melt$Species_Locus<-factor(bars.melt$Species_Locus,levels=rev(levels(bars.melt$Species_Locus)))

bars<-ggplot(bars.melt[order(bars.melt$variable,decreasing=T),],aes(x=Species_Locus, y=value, fill=variable)) + geom_col() + coord_flip() + scale_fill_manual(values=c("Grey50","Green","Blue","Yellow"))

bars
```

## Plot distance splines
```{r}
splinegrab<-function(ispline){
  x<-as.data.frame(ispline$x)
  y<-as.data.frame(ispline$y)
  xy<-as.data.frame(cbind(x$matrix_1,y$matrix_1))
}

distsplines<-lapply(isplines,splinegrab)
distsplines2<-melt(test,id.vars=c("V1","V2"))

s<-ggplot(distsplines2,aes(x=V1,y=V2,group=L1,color=L1)) + geom_line() + guides(color=FALSE)
s<-s + facet_grid(L1 ~ .)

```
# dbRDA

```{r dbRDA}
# read in the Fst/PhiSt table 
load("~/google_drive/DIPnet_Gait_Lig_Bird/DIPnet_WG4_first_papers/statistics/By_Species/Pairwise_statistics/sample/DIPnet_structure_sample_PhiST.Rdata")
#load("~/Desktop/DIPnet_structure_sample_PhiST_042817.Rdata")

# read in the best hypotheses
best_hypotheses<-read.csv("best_hypothesis.csv")

##1. Dataframe for results
stats<-data.frame(Species_Locus=character(0),constrained.inertia=numeric(0),totalInertia=numeric(0),ProportionConstrained=numeric(0),adj.R2.total=numeric(0),modelF=numeric(0),modelPvalue=numeric(0),pcx_Var=numeric(0),pcx_p=numeric(0),pcy_Var=numeric(0),pcy_p=numeric(0),bestmodel=character(0), constrained.inertia.best=numeric(0), total.inertia.best=numeric(0), proportion.constrained.inertia.best=numeric(0), adj.R2.best.model=numeric(0), stringsAsFactors = F)
stats$Species_Locus<-as.character(stats$Species_Locus)
#stats$Barrier<-as.character(stats$Barrier)
stats$bestmodel<-as.character(stats$bestmodel)

plotlist<-list()

# Make an empty list to save rda output for each species
esu_loci <- na4
all.gsl.rda<-sapply(esu_loci, function(x) NULL)

#make empty data frames to save variance and pvalues
term.vars<-data.frame(gsl=character(0))
term.pvals<-data.frame(gsl=character(0))

###############################################################################
# 2. Subsample for each species of interest, and filter based on Phi_ST table.
for(gsl in esu_loci){ #gsl<-"Linckia_laevigata_CO1" "Tridacna_crocea_CO1" "Lutjanus_kasmira_CYB"
  
  cat("\n","\n","\n","Now starting", gsl, "\n")
  
  #lookup the best hypothesis
  #best_h<-as.character(best_hypotheses[which(gsl==best_hypotheses$gsl),2])
  best_h<-"REALM"
#  if(any(is.na(diffstats[[gsl]]))){cat("NAs in FST table, No dbRDA calculated"); #all.gsl.rda[[gsl]]<-"NAs in FST table, No dbRDA calculated"; next}
 
  #replace NAs with 1s. NAs occur when no haplotypes are shared.
   if(any(is.na(diffstats[[gsl]]))){
    diffstats[[gsl]][which(is.na(diffstats[[gsl]]))]<-1
  }
   
  if(diffstats[[gsl]]=="Fewer than 3 sampled populations after filtering. No stats calculated"){all.gsl.rda[[gsl]]<-"Fewer than 4 sampled populations after filtering."; cat("Fewer than 4 sampled populations after filtering.");next}
  
   sp<-ipdb_ip[which(ipdb_ip$Genus_species_locus==gsl),]
  
  #clean weird backslashes from names
  sp$locality<-gsub("\"","",sp$locality)
  
  sp$sample<-paste(sp$locality,round(sp$decimalLatitude, digits=0),round(sp$decimalLongitude, digits=0),sep="_")  #sets up a variable that matches the name in Fst table
  sp<-sp[order(sp$sample),]
  # Not all localities are included in every realization - zap the ones that are NA
  #nonpops<-unique(sp$sample[is.na(sp$REALM)])
  sp<-sp[!is.na(sp[,best_h]),]
  
  #subsample Fst 
  gslFST<-diffstats[[gsl]]
  #make a matrix out of gslFST, convert negative values to zero
  gslFSTm<-as.matrix(gslFST)
  gslFSTm[which(gslFSTm<0)]<-0.0
  
  #zap weird slashes in the names
  rownames(gslFSTm)<-gsub("\"","",rownames(gslFSTm))
  colnames(gslFSTm)<-rownames(gslFSTm)
  
  #zap the same na populations from the list of non existent pops from VeronDivis
  # if(any(rownames(gslFSTm) %in% nonVeronpops)){
  #   gslFSTm<-gslFSTm[-(which(rownames(gslFSTm) %in% nonVeronpops)),-(which(colnames(gslFSTm) %in% nonVeronpops))]
  # }
  if(length(rownames(gslFSTm))<5){
    all.gsl.rda[[gsl]]<-"Fewer than 5 sampled populations"
    cat("Fewer than 5 sampled populations")
    next}
  
  #and filter sp based on the localities that have Fst values
  sp<-sp[sp$sample %in% rownames(gslFSTm),]
  
  #and vice versa
  
  gslFSTm<- gslFSTm[which(rownames(gslFSTm) %in% unique(sp$sample)),which(rownames(gslFSTm) %in% unique(sp$sample))]
  

  #if(length(rownames(gslFSTm))<5){all.gsl.rda[[gsl]]<-"Fewer than 5 sampled populations";cat("Fewer than 5 sampled populations");next}
  
  #create a locations data frame that has all the localities plus lats and longs and their Veron region.
  locs<-as.data.frame(unique(sp$sample))
  names(locs)<-"sample"
  #locs$Long<-sp$decimalLongitude[which(locs %in% sp$sample)]
  #can't do a unique on sample, lats and longs because some samples have non-unique lats and longs! So I do a join and take the first match.
  locs<-join(locs,sp[c("IPDB_index","sample","decimalLongitude","decimalLatitude","REALM","PROVINCE","ECOREGION","VeronDivis","Kulbicki_b","Kulbicki_r","Bowen","Keith")], by="sample", match="first")
  #if (length(unique(locs$REALM)) < 2) {"Only one region!"; cat("Only one region!"); next}
  
  #sort gslFSTm
  gslFSTm<-gslFSTm[order(rownames(gslFSTm)),order(colnames(gslFSTm))]

  
  ######################################################################
  # 4. Calculate Great Circle Distance
  gcdist_km <- pointDistance(locs[,3:4],lonlat=T)/1000
  # and symmetricise it
  gcdist_km[upper.tri(gcdist_km)]<-0
  gcdist_km<-gcdist_km + t(gcdist_km)
   
  ####################################################################### Calculate Overwater Distances#
  #Save for later##
   mySites<-locs$IPDB_index
  
  SubSet.Sites<-setDT(LookUp, key = 'IPDB_index')[list(mySites)]

  myIndex<-SubSet.Sites$Rid
  
  # some distances were not measurable due to point being 20km from water
  # zap these points from both matrices
  #rownames(mySubOutMat)<-mySites
  #colnames(mySubOutMat)<-mySites
  if(anyNA(myIndex)){
  nazaps<-which(is.na(myIndex))
  myIndex<-myIndex[-nazaps]
  gcdist_km<-gcdist_km[-nazaps,-(nazaps)]
  gslFSTm<-gslFSTm[-nazaps,-(nazaps)]
  locs<-locs[-nazaps,]
 
  }
  
  owdist_km<-outMat[myIndex,myIndex]
  
  #add the sample names
  rownames(owdist_km) <- locs$sample
  
 

  ##############################################################################
  #5. Create a matrix of regional identities
  if(length(unique(locs[,best_h]))==1){
    name<-gsub(unique(locs[,best_h]),pattern = "[ -]", replacement=".")
    name<-paste0("get.best_h.",name)
    regions<-data.frame(name=rep(1,length(locs$sample)))
    colnames(regions)<-name
    }
  else{
    regions<-with(locs, data.frame(model.matrix(~get(best_h)+0)))   #one of the predictors is superfluous but will get knocked out during RDA
    row.names(regions)<-row.names(locs)
  }
  
  #################################################################################
  #6. Plot IBD
  
  ibd<-as.data.frame(cbind(distance=as.dist(owdist_km), phist=as.dist(gslFSTm)))
  
  ibdplot<- ggplot(data=ibd,mapping=aes(x=distance,y=phist)) + geom_point() +
      geom_smooth(method=lm, color="purple") +
      ggtitle(label=gsl)

plotlist[[gsl]]<-ibdplot

    ############################################################################
    # 7. Calculate the principal coordinates
    
    FST.pcoa<-cmdscale(gslFSTm, k=dim(as.matrix(gslFSTm))[1] - 1, eig=TRUE, add=FALSE) #ignore warnings - OK to have negatives according to Anderson
    FST.scores<-FST.pcoa$points
    
    owdist.pcoa<-cmdscale(owdist_km, k=2, eig=TRUE, add=FALSE)
    owdist.scores<-owdist.pcoa$points
    owdist.scores<-data.frame("pcx"=owdist.pcoa$points[,1],"pcy"=owdist.pcoa$points[,2])
    locs2<-cbind(regions,owdist.scores)
    
    ###########################################################################
    # 8. Calculate the RDA and extract statistics
    RDA.res<-rda(FST.scores~., data=locs2, scale=TRUE )
    
    #Extract Statistics
    constrained.inertia<-summary(RDA.res)$constr.chi
    total.inertia<-summary(RDA.res)$tot.chi
    proportion.constrained.inertia<-constrained.inertia/total.inertia
    
    adj.R2.total.model<-RsquareAdj(RDA.res)$adj.r.squared
    if(is.na(adj.R2.total.model)){cat("Predictors >= Observations! No solution");all.gsl.rda[[gsl]]<-"Predictors >= Observations! No solution";next}
    
    model.sig<-anova.cca(RDA.res, step=1000)
    modelF<-model.sig$F[1]
    modelPvalue<-model.sig$`Pr(>F)`[1]
    
    ##Uncomment this section to harvest variances from the full model###
    # terms.sig<-anova(RDA.res, by="term", step=1000)
    # pcx_Var<-terms.sig$Variance[1]
    # pcx_p<-terms.sig$`Pr(>F)`[1]
    # pcy_Var<-terms.sig$Variance[2]
    # pcy_p<-terms.sig$`Pr(>F)`[2]
    # 
    # #extract all variance values into a dataframe
    # term.var<-as.data.frame(t(terms.sig[,2]))
    # names(term.var)<-rownames(terms.sig)
    # 
    # #extract all p-values into a dataframe
    # term.pval<-as.data.frame(t(terms.sig[,4]))
    # names(term.pval)<-rownames(terms.sig)
    # 
    #  
    # # scale the variance
    # term.var<-term.var/sum(term.var)
    # 
    # # set variance to zero if it is not significant
    # term.var[1,which(term.pval[1,] > 0.05 | is.na(term.pval[1,]))]<-0
    # term.var$Residual <- 1 - sum(term.var)
    # 
    # #set all values to zero if the model itself is not significant
    # #if(modelPvalue > 0.05){
    # #term.var[1,which(names(term.var)!="gsl")]<-0
    # #}
    # # add on the species name
    # term.pval$gsl<-gsl
    # term.var$gsl<-gsl
    # 
    # #merge them on to each dataframe
    # term.vars<-merge(term.vars,term.var,all=T)
    # term.pvals<-merge(term.pvals,term.pval,all=T)
    ##Uncomment this section to harvest variances from the full model###
    
    
    #barrier_Var<-terms.sig$Variance[3]   #omitting for now as the number of barriers will differ
    #barrier_p<-terms.sig$`Pr(>F)`[3]
    
    #marg.sig<-anova(RDA.res, by="margin", step=1000)
    #barrier_margVar<-marg.sig$Variance[3]
    #barrier_margp<-marg.sig$`Pr(>F)`[3]
    
    #Model selection
    nullmodel<-rda(FST.scores~1, data=locs2, scale=TRUE )
    forward.model<-ordiR2step(nullmodel, scope=formula(RDA.res), R2scope=FALSE, psteps=1000)  #ordiR2step implements Blanchets stopping criterion
    bestmodel<-as.character(forward.model$call[2])
    if (is.null(summary(forward.model)$constr.chi)) {
      constrained.inertia.best<-NA
      total.inertia.best<-summary(forward.model)$tot.chi
      proportion.constrained.inertia.best<-NA
      adj.R2.best.model<-NA
    } else {
      constrained.inertia.best<-summary(forward.model)$constr.chi
      total.inertia.best<-summary(forward.model)$tot.chi
      proportion.constrained.inertia.best<-constrained.inertia.best/total.inertia.best
      adj.R2.best.model<-RsquareAdj(forward.model)$adj.r.squared
    }

    
#### Uncomment this section to harvest variances from the model selection model
    
    terms.sig<-anova(forward.model, by="term", step=1000)
    pcx_Var<-terms.sig$Variance[1]
    pcx_p<-terms.sig$`Pr(>F)`[1]
    pcy_Var<-terms.sig$Variance[2]
    pcy_p<-terms.sig$`Pr(>F)`[2]
    
    #extract all variance values into a dataframe
    term.var<-as.data.frame(t(terms.sig[,2]))
    names(term.var)<-rownames(terms.sig)
   
    #extract all p-values into a dataframe
    term.pval<-as.data.frame(t(terms.sig[,4]))
    names(term.pval)<-rownames(terms.sig)
    
     
    # scale the variance
    term.var<-term.var/sum(term.var)
    
    # add on the species name
    term.pval$gsl<-gsl
    term.var$gsl<-gsl
    
    #merge them on to each dataframe
    term.vars<-merge(term.vars,term.var,all=T)
    term.pvals<-merge(term.pvals,term.pval,all=T)

####Uncomment this section to harvest variances from the model selection model ####
  
    #save stats
    
    stats_model<-c(gsl,constrained.inertia,total.inertia,proportion.constrained.inertia,adj.R2.total.model,modelF,modelPvalue,pcx_Var,pcx_p,pcy_Var,pcy_p, bestmodel, constrained.inertia.best, total.inertia.best, proportion.constrained.inertia.best, adj.R2.best.model)
    
    stats[nrow(stats)+1,]<-stats_model
    
    all.gsl.rda[[gsl]]<-forward.model
    
    
 
  }

write.csv(term.vars,file="./output/dbRDA_term_vars_PHIST_bowenrealms_041118.csv")
write.csv(term.pvals,file="./output/dbRDA_term_pvals_PHIST_bowenrealms_041118.csv")

 save(all.gsl.rda,file="./output/multibarrier_dbRDA_PHIST_bowenrealms_041118.Rdata")
  
write.csv(stats, "./output/multibarrier_dbRDA_PHIST_bowenrealms_041118.csv")

ibdplots<-marrangeGrob(plotlist, nrow=2,ncol=2)
ggsave("./output/ibdplots_PHIST_bowenrealms_041118.pdf",ibdplots)
  
```

## Make a heatmap of the variance values


```{r}


#melt for ggplot2
variance_melted<-melt(term.vars,id.vars = "gsl")
colnames(variance_melted)<-c("Species","Variable","Proportion_of_Variance")
variance_melted$Species<-as.factor(variance_melted$Species)


#baseplot
dbrda<-ggplot(variance_melted,aes(y=Species,x=Variable,fill=Proportion_of_Variance))

#add geom_tile, turn the x-axis elements by 90 degrees, reverse the names on the y-axis, and use a diverging color scheme to highlight hypotheses with >50% rel prob.
dbrda<-dbrda+geom_tile()+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+
  ylim(rev(levels(variance_melted$Species)))+
  scale_fill_gradientn(colors=c("white","pink","red"),values = c(0,0.000001,1),
                       na.value = "grey50", guide = "colourbar")

dbrda
```



# Stacked Bars for dbRDA

```{r}

#sum all non distance, non residual variance terms - these will be the structure component.

term.vars$Structure<-rowSums(term.vars[,-which(names(term.vars) %in% c("gsl","pcx","pcy","Residual"))],na.rm=TRUE)

#Subset just the values that will be plotted
term.vars.structure<-term.vars[,c("gsl","Residual","Structure","pcx","pcy")]

#term.vars.structure[which(stats$modelPvalue>0.05),2]<-1
#term.vars.structure[which(stats$modelPvalue>0.05),c(3,4,5)]<-0

#species names
spnames<-sub("(\\w+)_(\\w+)_\\w+","\\1 \\2",term.vars.structure$gsl)

#melt into long format
term.vars.melt<-melt(term.vars.structure, id.vars="gsl")

term.vars.melt$gsl<-factor(term.vars.melt$gsl)
term.vars.melt$gsl<-factor(term.vars.melt$gsl,levels=rev(levels(term.vars.melt$gsl)))

bars<-ggplot(data=term.vars.melt, mapping=aes(x=gsl,y=value,fill=variable)) +
  geom_col() + coord_flip() +
  scale_fill_manual(values=c("Grey50","Green","Blue","Blue4")) +
  scale_x_discrete(labels=rev(spnames)) + xlab(element_blank()) + ylab("Proportion")



bars

ggsave(filename = "./output/dbRDA_stacked_bars_PHI_bowenrealms_041118.pdf",bars,width=7,height=7,units="in")
```

# Structure as Lines on a Map

Now I am going to come up with code to measure the mean PhiST across the Spalding Realms




## Calculate the diffstats
```{r}
#diffstats2<-pairwise.structure.mtDNA.db(ipdb=ipdb_ip, gdist = "PhiST", minseqs = 5, minsamps = 2, mintotalseqs = 0, nrep = 1000, num.cores = 2, ABGD = F, regionalization = "REALM")
#save(diffstats_phist,file="./output/PHIst_for_map.R")
#save(diffstats_fst,file="./output/Fst_for_map.R")

#load("./output/PHIst_for_map.R")
#load("./output/FST_for_map.R")

# now 
str(ipdb2)

```

## Function to pull out pairwise values for putative barriers

```{r}
# first filter out all species that had fewer than 2 sampled populations
diffstats3<-diffstats_fst[lapply(diffstats2, class) =="dist"]


barrier_extract<-function(x){
  
  a<-as.matrix(x)
  
   if("Central Indo-Pacific" %in% rownames(a) && "Western Indo-Pacific" %in% rownames(a))
   {west_central<- a["Central Indo-Pacific","Western Indo-Pacific"]}
   else{west_central<- NA}
  
   if("Central Indo-Pacific" %in% rownames(a) && "Eastern Indo-Pacific" %in% rownames(a))
   {east_central<- a["Central Indo-Pacific","Eastern Indo-Pacific"]}
   else{east_central<- NA}
  
   if("Hawaii" %in% rownames(a) && "Eastern Indo-Pacific" %in% rownames(a))
   {east_hawaii<- a["Hawaii","Eastern Indo-Pacific"]}
   else{east_hawaii<- NA}
   
   if("Marquesas" %in% rownames(a) && "Eastern Indo-Pacific" %in% rownames(a))
   {east_marquesas<- a["Marquesas","Eastern Indo-Pacific"]}
   else{east_marquesas<- NA}
   

   return(c(west_central,east_central,east_hawaii,east_marquesas))
}
```


## Randomized Fst
Randomly sort each locality for each species into two regions and calculate Fst between them.

```{r}
ipdb3<-ipdb_ip[which(ipdb_ip$Genus_species_locus %in% names(diffstats3)),]

ipdb3$randomregion<-NA

random_diffstats2<-NULL
random_diffstats_all<-NULL
for(i in 1:10){
#randomly assign all localities to 1 or 2
  for(loc in unique(ipdb3$locality)){
    ipdb3$randomregion[which(ipdb3$locality==loc)]<-sample(c(1,2),size=1)
  }

random_diffstats<-pairwise.structure.mtDNA.db(ipdb=ipdb3, gdist = "WC Theta", minseqs = 5, minsamps = 2, mintotalseqs = 0, nrep = 0, num.cores = 2, ABGD = F, regionalization = "randomregion")

random_diffstats2<-random_diffstats[lapply(random_diffstats, class) =="dist"]

random_diffstats_all<-c(random_diffstats_all,unlist(random_diffstats2))
}

random_diffstats_all_2<-unlist(random_diffstats_all)
random_diffstats_all_2[which(random_diffstats_all_2 < 0)]<-0

random_diffstats_all_3<-data.frame(variable=factor("Randomized"), value = as.numeric(random_diffstats_all_2))

#save(random_diffstats_all,file="./output/Randomized_PHIst.R")
#save(random_diffstats_all,file="./output/Randomized_FST.R")
load("Randomized_PHIst.R")
```


## Pull them out and plot them

```{r}

extracted_Fst<-lapply(diffstats3,barrier_extract)

extracted_Fst2<-matrix(unlist(extracted_Fst), ncol=4, byrow=T)

rownames(extracted_Fst2)<-names(diffstats3)

colnames(extracted_Fst2)<-c("West_Central","East_Central","East_Hawaii","East_Marquesas")

#set values less than 0 to 0
extracted_Fst2[which(extracted_Fst2 < 0)] <- 0

extracted_Fst2<-as.data.frame(extracted_Fst2)


extracted_Fst_melt<-melt(extracted_Fst2,measure.vars=names(extracted_Fst2), na.rm=T)

#rbind on the randomized values
extracted_Fst_melt<-rbind(extracted_Fst_melt,random_diffstats_all)

#function to add in n values
n_fun <- function(x){
  return(data.frame(y = -0.05, label = paste0("n = ",length(x))))
}

#ggplot(data=extracted_Fst_melt, aes(x=variable,y=value)) + geom_boxplot() + stat_summary(fun.data = n_fun, geom = "text")

phist_violins<-ggplot(data=extracted_Fst_melt, aes(x=variable,y=value, alpha=variable)) + geom_violin(alpha=1) + stat_summary(fun.data = n_fun, geom = "text", alpha=1) + geom_jitter(size=0.0) + ylab(expression(Phi[ST])) + xlab("Barrier") + scale_alpha_manual(values=c(0.5,0.5,0.5,0.5,0), guide=F) + scale_x_discrete(labels=c("Western Indo-Pacific / \n Central Indo-Pacific", "Central Indo-Pacific / \n Eastern Indo-Pacific", "Eastern Indo-Pacific / \n Hawaii", "Eastern Indo-Pacific / \n Marquesas", "Randomized"))


#Function for 1000 bootstraps which take 100 samples from the data with replacement and establish 95%CI
bootstrap_phist<-function(x){
quantile(sapply(1:1000,function(y) median(sample(x=x,size=100,replace=T),na.rm=T)),probs=c(0.025,0.975))
}

medians<-c(apply(extracted_Fst2,MARGIN=2,FUN=median, na.rm=T), Randomized=median(random_diffstats_all$value,na.rm=T))

CI95<-cbind(apply(extracted_Fst2,MARGIN=2,FUN=bootstrap_phist),Randomized=bootstrap_phist(random_diffstats_all$value))

medianCI<-as.data.frame(t(rbind(medians,CI95)))
colnames(medianCI)<-c("medians","low","high")
medianCI["barrier"]<-factor(rownames(medianCI), levels=c("West_Central","East_Central","East_Hawaii","East_Marquesas","Randomized"))
medianCI

phist_violins2<-phist_violins  + geom_pointrange(data=medianCI,mapping=aes(x=barrier,y=medians,ymin=low,ymax=high), inherit.aes=F,fatten=1) + coord_cartesian(ylim=c(0,0.55))

phist_medians<-ggplot(data=medianCI) + geom_pointrange(data=medianCI,mapping=aes(x=barrier,y=medians,ymin=low,ymax=high), inherit.aes=F,fatten=4) + scale_x_discrete(labels=c("Western Indo-Pacific / \n Central Indo-Pacific", "Central Indo-Pacific / \n Eastern Indo-Pacific", "Eastern Indo-Pacific / \n Hawaii", "Eastern Indo-Pacific / \n Marquesas", "Randomized")) + ylab(expression(Phi[ST])) + xlab("Barrier")  + stat_summary(data=extracted_Fst_melt,mapping=aes(x=variable,y=value),fun.data = n_fun, geom = "text", alpha=1, position_fill(vjust=1.1))  + coord_cartesian(ylim=c(0,0.1)) + theme(axis.text.x = element_text(angle=45, hjust=1))
 
medianCI

ggsave(phist_medians,file="./output/phist_medians1.pdf", width=7, height=7, units="in")

```
